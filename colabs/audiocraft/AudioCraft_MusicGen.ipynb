{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-26KlXuiXul"
      },
      "source": [
        "# üé∏ Generating Music using MusicGen and W&B üêù\n",
        "\n",
        "In this notebook we demonstrate how you can generate music from text prompts or generate new music from existing music using the MusicGen model from [Audiocraft](https://github.com/facebookresearch/audiocraft) and play and visualize them using [Weights & Biases](https://wandb.ai/site)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "EZU3hg4B1om6"
      },
      "outputs": [],
      "source": [
        "# @title Install AudioCraft + WandB\n",
        "!pip install -q -U audiocraft wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "RerQaiZt14r8"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "import os\n",
        "import random\n",
        "from tempfile import TemporaryDirectory\n",
        "\n",
        "from scipy import signal\n",
        "from scipy.io import wavfile\n",
        "\n",
        "import torchaudio\n",
        "from audiocraft.models import MusicGen\n",
        "from audiocraft.data.audio import audio_write\n",
        "\n",
        "import wandb\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "from google.colab import files\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        },
        "id": "3MTX8GoE7AzN",
        "outputId": "73b043a0-ee19-4c27-ec47-3b9e556635b9"
      },
      "outputs": [],
      "source": [
        "# @title ## MusicGen Configs\n",
        "\n",
        "# @markdown WandB Project Name\n",
        "project_name = \"audiocraft\" # @param {type:\"string\"}\n",
        "\n",
        "wandb.init(project=project_name, job_type=\"musicgen/inference\")\n",
        "\n",
        "config = wandb.config\n",
        "\n",
        "# @markdown Select the MusicGen variant\n",
        "config.model_name = \"small\" # @param [\"small\", \"medium\", \"large\", \"melody\"]\n",
        "\n",
        "# @markdown ---\n",
        "# @markdown ## Conditional Generation Configs\n",
        "\n",
        "# @markdown The prompt for generating audio. You can give multiple prompts separated by `|` in the input. You can also leave it blank for unconditional generation.\n",
        "config.prompts = \"happy rock | energetic EDM | sad jazz\" # @param {type:\"string\"}\n",
        "\n",
        "descriptions = [prompt.strip() for prompt in config.prompts.split(\"|\")]\n",
        "config.is_unconditional = config.prompts.strip() == \"\"\n",
        "\n",
        "# @markdown **Note:** If you have provided prompts, you will be prompted to provide an audio file in addition to the prompts to condition the model. If you don't want to provide a file as an additional condition to the model, just press on the `cancel` button.\n",
        "input_audio, input_sampling_rate, wandb_input_audio = None, None, None\n",
        "if not config.is_unconditional:\n",
        "    input_audio_file = files.upload()\n",
        "    if input_audio_file != {}:\n",
        "        wandb_input_audio = wandb.Audio(list(input_audio_file.keys())[0])\n",
        "        input_audio, input_sampling_rate = torchaudio.load(\n",
        "            list(input_audio_file.keys())[0]\n",
        "        )\n",
        "        config.input_audio_available = True\n",
        "    else:\n",
        "        config.input_audio_available = False\n",
        "\n",
        "# @markdown Number of audio samples generated, this is relevant only for unconditional generation, i.e, if `config.prompts` is left blank.\n",
        "config.num_samples = 4 # @param {type:\"slider\", min:1, max:10, step:1}\n",
        "\n",
        "# @markdown Specify the random seed\n",
        "seed = None # @param {type:\"raw\"}\n",
        "\n",
        "max_seed = int(1024 * 1024 * 1024)\n",
        "if not isinstance(seed, int):\n",
        "    seed = random.randint(1, max_seed)\n",
        "if seed < 0:\n",
        "    seed = - seed\n",
        "seed = seed % max_seed\n",
        "config.seed = seed\n",
        "\n",
        "# @markdown ---\n",
        "# @markdown ## Generation Parameters\n",
        "# @markdown Use sampling if True, else do argmax decoding\n",
        "config.use_sampling = True # @param {type:\"boolean\"}\n",
        "\n",
        "# @markdown `top_k` used for sampling; limits us to `k` number of  of the top tokens to consider.\n",
        "config.top_k = 250 # @param {type:\"slider\", min:0, max:1000, step:1}\n",
        "\n",
        "# @markdown `top_p` used for sampling; limits us to the top tokens within a probability mass `p`\n",
        "config.top_p = 0.0 # @param {type:\"slider\", min:0, max:1.0, step:0.01}\n",
        "\n",
        "# @markdown Softmax temperature parameter\n",
        "config.temperature = 1.0 # @param {type:\"slider\", min:0, max:1.0, step:0.01}\n",
        "\n",
        "# @markdown Duration of the generated waveform\n",
        "config.duration = 10 # @param {type:\"slider\", min:1, max:30, step:1}\n",
        "\n",
        "# @markdown Coefficient used for classifier free guidance\n",
        "config.cfg_coef = 3 # @param {type:\"slider\", min:1, max:100, step:1}\n",
        "\n",
        "# @markdown Whether to perform 2 forward for Classifier Free Guidance instead of batching together the two. This has some impact on how things are padded but seems to have little impact in practice.\n",
        "config.two_step_cfg = False # @param {type:\"boolean\"}\n",
        "\n",
        "# @markdown When doing extended generation (i.e. more than 30 seconds), by how much should we extend the audio each time. Larger values will mean less context is preserved, and shorter value will require extra computations.\n",
        "config.extend_stride = 18 # @param {type:\"slider\", min:1, max:30, step:1}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SfM8rhVX7ES9",
        "outputId": "cd895a2c-daa1-4043-e217-a90573d8d8ab"
      },
      "outputs": [],
      "source": [
        "# @title Generate Audio using MusicGen\n",
        "\n",
        "model = MusicGen.get_pretrained(config.model_name)\n",
        "model.set_generation_params(\n",
        "    use_sampling=config.use_sampling,\n",
        "    top_k=config.top_k,\n",
        "    top_p=config.top_p,\n",
        "    temperature=config.temperature,\n",
        "    duration=config.duration,\n",
        "    cfg_coef=config.cfg_coef,\n",
        "    two_step_cfg=config.two_step_cfg,\n",
        "    extend_stride=config.extend_stride\n",
        ")\n",
        "\n",
        "generated_wav = None\n",
        "if config.is_unconditional:\n",
        "    if input_audio is None:\n",
        "        generated_wav = model.generate_unconditional(\n",
        "            num_samples=config.num_samples, progress=True\n",
        "        )\n",
        "    else:\n",
        "        generated_wav = model.generate_with_chroma(\n",
        "            descriptions,\n",
        "            input_audio[None].expand(3, -1, -1),\n",
        "            input_sampling_rate\n",
        "        )\n",
        "else:\n",
        "    generated_wav = model.generate(descriptions, progress=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "_n-1RthFVPYN"
      },
      "outputs": [],
      "source": [
        "# @title Log Audio to Weights & Biases Dashboard\n",
        "\n",
        "def get_spectrogram(audio_file, output_file):\n",
        "    sample_rate, samples = wavfile.read(audio_file)\n",
        "    frequencies, times, Sxx = signal.spectrogram(samples, sample_rate)\n",
        "\n",
        "    log_Sxx = 10 * np.log10(Sxx + 1e-10)\n",
        "    vmin = np.percentile(log_Sxx, 5)\n",
        "    vmax = np.percentile(log_Sxx, 95)\n",
        "\n",
        "    mean_spectrum = np.mean(log_Sxx, axis=1)\n",
        "    threshold_low = np.percentile(mean_spectrum, 5)\n",
        "    threshold_high = np.percentile(mean_spectrum, 95)\n",
        "\n",
        "    freq_indices = np.where(mean_spectrum > threshold_low)\n",
        "    freq_min = 20\n",
        "    freq_max = frequencies[freq_indices].max()\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    cmap = plt.get_cmap('magma')\n",
        "\n",
        "    ax.pcolormesh(\n",
        "        times,\n",
        "        frequencies,\n",
        "        log_Sxx,\n",
        "        shading='gouraud',\n",
        "        cmap=cmap,\n",
        "        vmin=vmin,\n",
        "        vmax=vmax\n",
        "    )\n",
        "    ax.axis('off')\n",
        "    ax.set_ylim([freq_min, freq_max])\n",
        "\n",
        "    plt.subplots_adjust(left=0, right=1, top=1, bottom=0)\n",
        "    plt.savefig(\n",
        "        output_file, format='png', bbox_inches='tight', pad_inches=0\n",
        "    )\n",
        "    plt.close()\n",
        "\n",
        "    return wandb.Image(output_file)\n",
        "\n",
        "\n",
        "temp_dir = TemporaryDirectory()\n",
        "columns = [\"Prompt\", \"Generated-Audio\", \"Spectrogram\", \"Seed\"]\n",
        "if input_audio is not None:\n",
        "    columns.insert(1, \"Input-Audio\")\n",
        "wandb_table = wandb.Table(columns=columns)\n",
        "\n",
        "for idx, wav in enumerate(generated_wav):\n",
        "    file_name = os.path.join(temp_dir.name, str(idx))\n",
        "    audio_write(\n",
        "        file_name,\n",
        "        wav.cpu(),\n",
        "        model.sample_rate,\n",
        "        strategy=\"loudness\",\n",
        "        loudness_compressor=True,\n",
        "    )\n",
        "    wandb_audio = wandb.Audio(file_name +  \".wav\")\n",
        "    wandb.log({\"Generated-Audio\": wandb_audio})\n",
        "    desc = descriptions[idx] if len(descriptions) > 1 else config.prompts\n",
        "    wandb_table_row = [\n",
        "        desc,\n",
        "        wandb_audio,\n",
        "        get_spectrogram(\n",
        "            audio_file=file_name +  \".wav\",\n",
        "            output_file=os.path.join(temp_dir.name, str(idx) + \".png\")\n",
        "        ),\n",
        "        config.seed\n",
        "    ]\n",
        "    if input_audio is not None:\n",
        "        wandb_table_row.insert(1, wandb_input_audio)\n",
        "    wandb_table.add_data(*wandb_table_row)\n",
        "\n",
        "wandb.log({\"Generated-Audio-Table\": wandb_table})\n",
        "\n",
        "wandb.finish()\n",
        "temp_dir.cleanup()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
