{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/wandb/examples/blob/master/colabs/openai/Fine_tune_GPT_3_with_Weights_&_Biases.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "<!--- @wandbcode{openai-finetune-gpt3} -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://wandb.me/logo-im-png\" width=\"400\" alt=\"Weights & Biases\" />\n",
    "\n",
    "<!--- @wandbcode{openai-finetune-gpt35} -->\n",
    "\n",
    "# Fine-tune ChatGPT-3.5 and GPT-4 with Weights & Biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you use OpenAI's API to [fine-tune ChatGPT-3.5](https://platform.openai.com/docs/guides/fine-tuning), you can now use the `WandbLogger` integration to track experiments, models, and datasets in your central dashboard with just two lines of code:\n",
    "\n",
    "```\n",
    "from wandb.integration.openai import WandbLogger\n",
    "\n",
    "# Your fine-tuning logic\n",
    "\n",
    "WandbLogger.sync(id=fine_tune_job_id)\n",
    "```\n",
    "\n",
    "See the [OpenAI section](https://wandb.me/openai-docs) in the Weights & Biases documentation for full details of the integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -Uq openai tiktoken datasets tenacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove once this PR: https://github.com/wandb/wandb/pull/6498 is merged\n",
    "!pip install git+https://github.com/wandb/wandb.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this colab notebook, we will be finetuning GPT 3.5 model on the [LegalBench](https://hazyresearch.stanford.edu/legalbench/) dataset. The notebook will show how to prepare and validate the dataset, upload it to OpenAI and setup a fine-tune job. Finally, the notebook shows how to use the `WandbLogger`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and initial set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import wandb\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import random\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from collections import defaultdict\n",
    "from tenacity import retry, stop_after_attempt, wait_fixed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the OpenAI client\n",
    "\n",
    "You can add the api key to your environment variable by doing `os.environ['OPENAI_API_KEY'] = \"sk-....\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the line below and set your OpenAI API Key.\n",
    "# os.environ['OPENAI_API_KEY'] = \"sk-....\" \n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the `WandbLogger`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wandb.integration.openai.fine_tuning import WandbLogger\n",
    "\n",
    "WANDB_PROJECT = \"OpenAI-Fine-Tune\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Preparation\n",
    "\n",
    "We download a dataset from [LegalBench](https://hazyresearch.stanford.edu/legalbench/), a project to curate tasks for evaluating legal reasoning, specifically the [Contract NLI Explicit Identification task](https://github.com/HazyResearch/legalbench/tree/main/tasks/contract_nli_explicit_identification).\n",
    "\n",
    "This comprises of a total of 117 examples, from which we will create our own train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Download the data, merge into a single dataset and shuffle\n",
    "dataset = load_dataset(\"nguha/legalbench\", \"contract_nli_explicit_identification\")\n",
    "\n",
    "data = []\n",
    "for d in dataset[\"train\"]:\n",
    "  data.append(d)\n",
    "\n",
    "for d in dataset[\"test\"]:\n",
    "  data.append(d)\n",
    "\n",
    "random.shuffle(data)\n",
    "\n",
    "for idx, d in enumerate(data):\n",
    "  d[\"new_index\"] = idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a few samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data), data[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format our Data for Chat Completion Models\n",
    "We modify the `base_prompt` from the LegalBench task to make it a zero-shot prompt, as we are training the model instead of using few-shot prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_prompt_zero_shot = \"Identify if the clause provides that all Confidential Information shall be expressly identified by the Disclosing Party. Answer with only `Yes` or `No`\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now split it into training/validation dataset, lets train on 30 samples and test on the remainder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = 30\n",
    "n_test = len(data) - n_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_messages = []\n",
    "test_messages = []\n",
    "\n",
    "for d in data:\n",
    "  prompts = []\n",
    "  prompts.append({\"role\": \"system\", \"content\": base_prompt_zero_shot})\n",
    "  prompts.append({\"role\": \"user\", \"content\": d[\"text\"]})\n",
    "  prompts.append({\"role\": \"assistant\", \"content\": d[\"answer\"]})\n",
    "\n",
    "  if int(d[\"new_index\"]) < n_train:\n",
    "    train_messages.append({'messages': prompts})\n",
    "  else:\n",
    "    test_messages.append({'messages': prompts})\n",
    "\n",
    "len(train_messages), len(test_messages), n_test, train_messages[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the data to Weights & Biases\n",
    "\n",
    "Save the data in a train and test file first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_path = 'encoded_train_data.jsonl'\n",
    "with open(train_file_path, 'w') as file:\n",
    "    for item in train_messages:\n",
    "        line = json.dumps(item)\n",
    "        file.write(line + '\\n')\n",
    "\n",
    "test_file_path = 'encoded_test_data.jsonl'\n",
    "with open(test_file_path, 'w') as file:\n",
    "    for item in test_messages:\n",
    "        line = json.dumps(item)\n",
    "        file.write(line + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the OpenAI data validation script\n",
    "Next, we validate that our training data is in the correct format using a script from the [OpenAI fine-tuning documentation](https://platform.openai.com/docs/guides/fine-tuning/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we specify the data path and open the JSONL file\n",
    "\n",
    "def openai_validate_data(dataset_path):\n",
    "  data_path = dataset_path\n",
    "\n",
    "  # Load dataset\n",
    "  with open(data_path) as f:\n",
    "      dataset = [json.loads(line) for line in f]\n",
    "\n",
    "  # We can inspect the data quickly by checking the number of examples and the first item\n",
    "\n",
    "  # Initial dataset stats\n",
    "  print(\"Num examples:\", len(dataset))\n",
    "  print(\"First example:\")\n",
    "  for message in dataset[0][\"messages\"]:\n",
    "      print(message)\n",
    "\n",
    "  # Now that we have a sense of the data, we need to go through all the different examples and check to make sure the formatting is correct and matches the Chat completions message structure\n",
    "\n",
    "  # Format error checks\n",
    "  format_errors = defaultdict(int)\n",
    "\n",
    "  for ex in dataset:\n",
    "      if not isinstance(ex, dict):\n",
    "          format_errors[\"data_type\"] += 1\n",
    "          continue\n",
    "\n",
    "      messages = ex.get(\"messages\", None)\n",
    "      if not messages:\n",
    "          format_errors[\"missing_messages_list\"] += 1\n",
    "          continue\n",
    "\n",
    "      for message in messages:\n",
    "          if \"role\" not in message or \"content\" not in message:\n",
    "              format_errors[\"message_missing_key\"] += 1\n",
    "\n",
    "          if any(k not in (\"role\", \"content\", \"name\") for k in message):\n",
    "              format_errors[\"message_unrecognized_key\"] += 1\n",
    "\n",
    "          if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\"):\n",
    "              format_errors[\"unrecognized_role\"] += 1\n",
    "\n",
    "          content = message.get(\"content\", None)\n",
    "          if not content or not isinstance(content, str):\n",
    "              format_errors[\"missing_content\"] += 1\n",
    "\n",
    "      if not any(message.get(\"role\", None) == \"assistant\" for message in messages):\n",
    "          format_errors[\"example_missing_assistant_message\"] += 1\n",
    "\n",
    "  if format_errors:\n",
    "      print(\"Found errors:\")\n",
    "      for k, v in format_errors.items():\n",
    "          print(f\"{k}: {v}\")\n",
    "  else:\n",
    "      print(\"No errors found\")\n",
    "\n",
    "  # Beyond the structure of the message, we also need to ensure that the length does not exceed the 4096 token limit.\n",
    "\n",
    "  # Token counting functions\n",
    "  encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "  # not exact!\n",
    "  # simplified from https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n",
    "  def num_tokens_from_messages(messages, tokens_per_message=3, tokens_per_name=1):\n",
    "      num_tokens = 0\n",
    "      for message in messages:\n",
    "          num_tokens += tokens_per_message\n",
    "          for key, value in message.items():\n",
    "              num_tokens += len(encoding.encode(value))\n",
    "              if key == \"name\":\n",
    "                  num_tokens += tokens_per_name\n",
    "      num_tokens += 3\n",
    "      return num_tokens\n",
    "\n",
    "  def num_assistant_tokens_from_messages(messages):\n",
    "      num_tokens = 0\n",
    "      for message in messages:\n",
    "          if message[\"role\"] == \"assistant\":\n",
    "              num_tokens += len(encoding.encode(message[\"content\"]))\n",
    "      return num_tokens\n",
    "\n",
    "  def print_distribution(values, name):\n",
    "      print(f\"\\n#### Distribution of {name}:\")\n",
    "      print(f\"min / max: {min(values)}, {max(values)}\")\n",
    "      print(f\"mean / median: {np.mean(values)}, {np.median(values)}\")\n",
    "      print(f\"p5 / p95: {np.quantile(values, 0.1)}, {np.quantile(values, 0.9)}\")\n",
    "\n",
    "  # Last, we can look at the results of the different formatting operations before proceeding with creating a fine-tuning job:\n",
    "\n",
    "  # Warnings and tokens counts\n",
    "  n_missing_system = 0\n",
    "  n_missing_user = 0\n",
    "  n_messages = []\n",
    "  convo_lens = []\n",
    "  assistant_message_lens = []\n",
    "\n",
    "  for ex in dataset:\n",
    "      messages = ex[\"messages\"]\n",
    "      if not any(message[\"role\"] == \"system\" for message in messages):\n",
    "          n_missing_system += 1\n",
    "      if not any(message[\"role\"] == \"user\" for message in messages):\n",
    "          n_missing_user += 1\n",
    "      n_messages.append(len(messages))\n",
    "      convo_lens.append(num_tokens_from_messages(messages))\n",
    "      assistant_message_lens.append(num_assistant_tokens_from_messages(messages))\n",
    "\n",
    "  print(\"Num examples missing system message:\", n_missing_system)\n",
    "  print(\"Num examples missing user message:\", n_missing_user)\n",
    "  print_distribution(n_messages, \"num_messages_per_example\")\n",
    "  print_distribution(convo_lens, \"num_total_tokens_per_example\")\n",
    "  print_distribution(assistant_message_lens, \"num_assistant_tokens_per_example\")\n",
    "  n_too_long = sum(l > 4096 for l in convo_lens)\n",
    "  print(f\"\\n{n_too_long} examples may be over the 4096 token limit, they will be truncated during fine-tuning\")\n",
    "\n",
    "  # Pricing and default n_epochs estimate\n",
    "  MAX_TOKENS_PER_EXAMPLE = 4096\n",
    "\n",
    "  MIN_TARGET_EXAMPLES = 100\n",
    "  MAX_TARGET_EXAMPLES = 25000\n",
    "  TARGET_EPOCHS = 3\n",
    "  MIN_EPOCHS = 1\n",
    "  MAX_EPOCHS = 25\n",
    "\n",
    "  n_epochs = TARGET_EPOCHS\n",
    "  n_train_examples = len(dataset)\n",
    "  if n_train_examples * TARGET_EPOCHS < MIN_TARGET_EXAMPLES:\n",
    "      n_epochs = min(MAX_EPOCHS, MIN_TARGET_EXAMPLES // n_train_examples)\n",
    "  elif n_train_examples * TARGET_EPOCHS > MAX_TARGET_EXAMPLES:\n",
    "      n_epochs = max(MIN_EPOCHS, MAX_TARGET_EXAMPLES // n_train_examples)\n",
    "\n",
    "  n_billing_tokens_in_dataset = sum(min(MAX_TOKENS_PER_EXAMPLE, length) for length in convo_lens)\n",
    "  print(f\"Dataset has ~{n_billing_tokens_in_dataset} tokens that will be charged for during training\")\n",
    "  print(f\"By default, you'll train for {n_epochs} epochs on this dataset\")\n",
    "  print(f\"By default, you'll be charged for ~{n_epochs * n_billing_tokens_in_dataset} tokens\")\n",
    "  print(\"See pricing page to estimate total costs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validate train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_validate_data(train_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validate test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_validate_data(test_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload the training and validation data to OpenAI\n",
    "\n",
    "We will first upload the data to OpenAI. This might take a few minutes depending on the size of your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_train_file_info = client.files.create(\n",
    "    file=open(train_file_path, \"rb\"), purpose=\"fine-tune\"\n",
    ")\n",
    "\n",
    "openai_valid_file_info = client.files.create(\n",
    "    file=open(test_file_path, \"rb\"), purpose=\"fine-tune\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_train_file_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_valid_file_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Notice the unique ids for both training and validation data. OpenAI uses these ids to access the uploaded data to fine-tune GPT 3.5 on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model and log to Weights & Biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define our ChatGPT-3.5 fine-tuning hyper-parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'gpt-3.5-turbo'\n",
    "n_epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_ft_job_info = client.fine_tuning.jobs.create(\n",
    "    training_file=openai_train_file_info.id,\n",
    "    model=model,\n",
    "    hyperparameters={\"n_epochs\": n_epochs},\n",
    "    validation_file=openai_valid_file_info.id\n",
    ")\n",
    "\n",
    "ft_job_id = openai_ft_job_info.id\n",
    "\n",
    "# Log to Weights and Biases\n",
    "WandbLogger.sync(fine_tune_job_id=ft_job_id, project=WANDB_PROJECT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> this takes around 5 minutes to train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thats it!**\n",
    "\n",
    "Now your model is training on OpenAI's machines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logging the fine-tuning job to W&B is straight forward. The integration will automatically log the following to W&B:\n",
    "\n",
    "- training and validation metrics (if validation data is provided)\n",
    "- log the training and validation data as W&B Tables for storage and versioning\n",
    "- log the fine-tuned model's metadata.\n",
    "\n",
    "The integration automatically creates the DAG lineage between the data and the model.\n",
    "\n",
    "> You can call the `WandbLogger` with the job id. The cell will keep running till the fine-tuning job is not complete. Once the job's status is `succeeded`, the `WandbLogger` will log metrics and data to W&B. This way you don't have to wait for the fine-tune job to be completed to call `WandbLogger.sync`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling `WandbLogger.sync` without any id will log all un-synced fine-tuned jobs to W&B\n",
    "\n",
    "See the [OpenAI section](https://wandb.me/openai-docs) in the Weights & Biases documentation for full details of the integration"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fine-tuning job is now successfully synced to Weights and Biases. Click on the URL above to open the [W&B run page](https://docs.wandb.ai/guides/app/pages/run-page). The following will be logged to W&B:\n",
    "\n",
    "#### Training and validation metrics\n",
    "\n",
    "![image.png](assets/metrics.png)\n",
    "\n",
    "#### Training and validation data as W&B Tables\n",
    "\n",
    "![image.png](assets/datatable.png)\n",
    "\n",
    "#### The data and model artifacts for version control (go to the overview tab)\n",
    "\n",
    "![image.png](assets/artifacts.png)\n",
    "\n",
    "#### The configuration and hyperparameters (go to the overview tab)\n",
    "\n",
    "![image.png](assets/configs.png)\n",
    "\n",
    "#### The data and model DAG\n",
    "\n",
    "![image.png](assets/dag.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run evalution and log the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best way to evaluate a generative model is to explore sample predictions from your evaluation set.\n",
    "\n",
    "Let's generate a few inference samples and log them to W&B and see how the performance compares to a baseline ChatGPT-3.5 model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be evaluating using the validation dataset. In the overview tab of the run page, find the \"validation_files\" in the Artifact Inputs section. Clicking on it will take you to the artifacts page. Copy the artifact URI (full name) as shown in the image below.\n",
    "\n",
    "![image](assets/select_artifact_uri.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(\n",
    "    project=WANDB_PROJECT,\n",
    "    job_type='eval'\n",
    ")\n",
    "\n",
    "VALIDATION_FILE_ARTIFACT_URI = '<entity>/<project>/valid-file-*' # REPLACE THIS WITH YOUR OWN ARTIFACT URI\n",
    "\n",
    "artifact_valid = run.use_artifact(\n",
    "    VALIDATION_FILE_ARTIFACT_URI,\n",
    "    type='validation_files'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code snippet below, download the logged validation data and prepare a pandas dataframe from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "artifact_valid_path = artifact_valid.download()\n",
    "print(\"Downloaded the validation data at: \", artifact_valid_path)\n",
    "\n",
    "validation_file = glob.glob(f\"{artifact_valid_path}/*.table.json\")[0]\n",
    "with open(validation_file, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "validation_df = pd.DataFrame(columns=data[\"columns\"], data=data[\"data\"])\n",
    "\n",
    "print(f\"There are {len(validation_df)} validation examples\")\n",
    "run.config.update({\"num_validation_samples\":len(validation_df)})\n",
    "\n",
    "validation_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to package the data in the dataframe in the format acceptable by GPT 3.5. The format is:\n",
    "\n",
    "```\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"some system prompt\"}, {\"role\": \"user\", \"content\": \"some user prompt\"}, {\"role\": \"assistant\", \"content\": \"completion text\"}]}\n",
    "```\n",
    "\n",
    "For evaluation we don't need to pack the `{\"role\": \"assistant\", \"content\": \"completition text\"}` in `messages` as this is meant to be generated by GPT 3.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_data_format(row):\n",
    "    role_system_content = row[\"role: system\"]\n",
    "    role_system_dict = {\"role\": \"system\", \"content\": role_system_content}\n",
    "\n",
    "    role_user_content = row[\"role: user\"]\n",
    "    role_user_dict = {\"role\": \"user\", \"content\": role_user_content}\n",
    "    \n",
    "    return [role_system_dict, role_user_dict]\n",
    "\n",
    "validation_df[\"messages\"] = validation_df.apply(lambda row: eval_data_format(row), axis=1)\n",
    "validation_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run evaluation on the Fine-Tuned Model\n",
    "\n",
    "Next up we will get the fine-tuned model's id from the logged `model_metadata`. In the overview tab of the run page, find the \"model\" in the Artifact Outputs section. Clicking on it will take you to the artifacts page. Copy the artifact URI (full name) as shown in the image below.\n",
    "\n",
    "![image](assets/select_model_artifact.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ARTIFACT_URI = '<entity>/<project>/model_metadata:v*' # REPLACE THIS WITH YOUR OWN ARTIFACT URI\n",
    "\n",
    "model_artifact = run.use_artifact(\n",
    "    MODEL_ARTIFACT_URI,\n",
    "    type='model'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_metadata_path = model_artifact.download()\n",
    "print(\"Downloaded the validation data at: \", model_metadata_path)\n",
    "\n",
    "model_metadata_file = glob.glob(f\"{model_metadata_path}/*.json\")[0]\n",
    "with open(model_metadata_file, 'r') as file:\n",
    "    model_metadata = json.load(file)\n",
    "\n",
    "model_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned_model = model_metadata[\"fine_tuned_model\"]\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run evaluation and log results to W&B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_table = wandb.Table(columns=['messages', 'completion', 'target'])\n",
    "\n",
    "eval_data = []\n",
    "\n",
    "for idx, row in tqdm(validation_df.iterrows()):\n",
    "    messages = row.messages\n",
    "    target = row[\"role: assistant\"]\n",
    "\n",
    "    res = client.chat.completions.create(model=fine_tuned_model, messages=messages, max_tokens=10)\n",
    "    completion = res.choices[0].message.content\n",
    "\n",
    "    eval_data.append([messages, completion, target])\n",
    "    prediction_table.add_data(messages[1]['content'], completion, target)\n",
    "\n",
    "wandb.log({'predictions': prediction_table})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the accuracy of the fine-tuned model and log to W&B\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "for e in eval_data:\n",
    "  if e[1].lower() == e[2].lower():\n",
    "    correct+=1\n",
    "\n",
    "accuracy = correct / len(eval_data)\n",
    "\n",
    "print(f\"Accuracy is {accuracy}\")\n",
    "wandb.log({\"eval/accuracy\": accuracy})\n",
    "wandb.summary[\"eval/accuracy\"] = accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run evaluation on a Baseline model for comparison\n",
    "Lets compare our model to the baseline model, `gpt-3.5-turbo`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_prediction_table = wandb.Table(columns=['messages', 'completion', 'target'])\n",
    "\n",
    "baseline_eval_data = []\n",
    "\n",
    "for idx, row in tqdm(validation_df.iterrows()):\n",
    "    messages = row.messages\n",
    "    target = row[\"role: assistant\"]\n",
    "\n",
    "    res = client.chat.completions.create(model=\"gpt-3.5-turbo\", messages=messages, max_tokens=10)\n",
    "    completion = res.choices[0].message.content\n",
    "\n",
    "    baseline_eval_data.append([messages, completion, target])\n",
    "    baseline_prediction_table.add_data(messages[1]['content'], completion, target)\n",
    "\n",
    "wandb.log({'baseline_predictions': baseline_prediction_table})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the accuracy of the fine-tuned model and log to W&B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_correct = 0\n",
    "for e in baseline_eval_data:\n",
    "  if e[1].lower() == e[2].lower():\n",
    "    baseline_correct+=1\n",
    "\n",
    "baseline_accuracy = baseline_correct / len(baseline_eval_data)\n",
    "print(f\"Baseline Accurcy is: {baseline_accuracy}\")\n",
    "wandb.log({\"eval/baseline_accuracy\": baseline_accuracy})\n",
    "wandb.summary[\"eval/baseline_accuracy\"] =  baseline_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And thats it! In this example we have prepared our data, logged it to Weights & Biases, fine-tuned an OpenAI model using that data, logged the results to Weights & Biases and then run evaluation on the fine-tuned model.\n",
    "\n",
    "From here you can start to train on larger or more complex tasks, or else explore other ways to modify ChatGPT-3.5 such as giving it a different tone and style or response.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources\n",
    "\n",
    "* [OpenAI Fine-Tuning Guide](https://platform.openai.com/docs/guides/fine-tuning)\n",
    "* [W&B Integration with OpenAI API Documentation](https://wandb.me/openai-docs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
