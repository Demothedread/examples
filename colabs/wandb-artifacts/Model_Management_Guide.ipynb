{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/wandb/examples/blob/master/colabs/wandb-artifacts/Model_Management_Guide.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W&B Model Management Guide Companion Notebook\n",
    "\n",
    "This is a companion notebook to the [W&B Model Management Guide](https://docs.wandb.ai/guides/models).\n",
    "\n",
    "**Table of Contents**\n",
    "* **Cell 1**: Installs `wandb` python library\n",
    "* **Cell 2** (Form): Allows you to specify some parameters and defines a handful of helper functions. Note: there is not any `wandb` specific library calls in these helper functions - they are purely used to allow the example cells to be more terse and focus on the key aspects of Model Management\n",
    "* **Cell 3**: (Train, Log, & Link Models) Covers steps [2. Traing & log a Model](https://docs.wandb.ai/guides/models#2.-train-and-log-model-versions) and [3. Link Model Versions to the Collection](https://docs.wandb.ai/guides/models#3.-link-model-versions-to-the-portfolio)\n",
    "* **Cell 4**: (Use, Evaluate, and Promote a Model) Covers steps [4. Using a Model Version](https://docs.wandb.ai/guides/models#4.-use-a-model-version), [5. Evaluate Model Performance](https://docs.wandb.ai/guides/models#5.-evaluate-model-performance), and [6. Promote a Version to Production](https://docs.wandb.ai/guides/models#6.-promote-a-version-to-production)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "**Stop! ðŸ›‘** Please complete [Step 1 of the tutorial](https://docs.wandb.ai/guides/models/walkthrough#1-create-a-new-registered-model) before continuing. This will ensure you have a **Model Collection** defined in your project. Enter the Project name where you created the Collection in `project_name` and the name of the Collection in the `model_collection_name` fields respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wandb -U -qqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = \"model-registry\" #@param {type:\"string\"}\n",
    "# dataset_name = \"mnist\" #@param {type:\"string\"}\n",
    "dataset_name = \"mnist\"\n",
    "model_collection_name = \"MNIST Grayscale 28x28\" #@param {type:\"string\"}\n",
    "import wandb\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        output = self.fc2(x)\n",
    "        return output\n",
    "\n",
    "def _sample_mnist(split0, split1, is_train=True):\n",
    "    \"Sample MNIST dataset\"\n",
    "    mnist_data = datasets.MNIST(\n",
    "        \"train_data/\" if is_train else \"test_data/\", \n",
    "        download  = True, \n",
    "        train = is_train,\n",
    "        transform = transforms.Compose([\n",
    "          transforms.ToTensor(), \n",
    "          ])\n",
    "        )\n",
    "\n",
    "    extra = len(mnist_data) - split0 - split1\n",
    "    assert extra >= 0\n",
    "    splits = torch.utils.data.random_split(mnist_data, [split0, split1, extra])\n",
    "    return splits[0], splits[1]\n",
    "\n",
    "def build_train_data(train_size, val_size, batch_size=128):\n",
    "    splits = _sample_mnist(train_size, val_size)\n",
    "    return DataLoader(splits[0], batch_size=batch_size), DataLoader(splits[1], batch_size=val_size)\n",
    "\n",
    "def build_test_data(test_size):\n",
    "    splits = _sample_mnist(test_size, 0, is_train=False)\n",
    "    return DataLoader(splits[0], batch_size=test_size)\n",
    "\n",
    "def build_model(learning_rate, total_steps):\n",
    "    device = torch.device(\"cpu\")\n",
    "    model = Net().to(device)\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    scheduler = OneCycleLR(optimizer, max_lr=learning_rate, total_steps=total_steps)\n",
    "    return model, optimizer, scheduler\n",
    "\n",
    "def train_step(model, optimizer, scheduler, batch_x, batch_y):\n",
    "    model.train()\n",
    "    batch_x, batch_y = batch_x.to(\"cpu\"), batch_y.to(\"cpu\")\n",
    "    optimizer.zero_grad()\n",
    "    preds = model(batch_x)\n",
    "    loss = F.cross_entropy(preds, batch_y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    return loss.item(), preds\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_model(model, eval_dl):\n",
    "    device = torch.device(\"cpu\")\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    preds = []\n",
    "    for data, target in eval_dl:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data)\n",
    "        test_loss += F.cross_entropy(\n",
    "            output, target, reduction=\"sum\"\n",
    "        ).item()  # sum up batch loss\n",
    "        pred = output.argmax(\n",
    "            dim=1, keepdim=True\n",
    "        )  # get the index of the max log-probability\n",
    "        preds += list(pred.flatten().tolist())\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    test_loss /= len(eval_dl.dataset)\n",
    "    accuracy = 100.0 * correct / len(eval_dl.dataset)\n",
    "    return test_loss, accuracy, preds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train, Log, & Link Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, is_best=False):\n",
    "    \"\"\"Save model to W&B and locally if it's the best one so far.\"\"\"\n",
    "    ##### W&B MODEL MANAGEMENT SPECIFIC CALLS ######\n",
    "    art = wandb.Artifact(f\"mnist-{wandb.run.id}\", \"model\")\n",
    "    torch.save(model.state_dict(), \"model.pt\")\n",
    "    art.add_file(\"model.pt\")\n",
    "    wandb.log_artifact(art, aliases=[\"best\", \"latest\"] if is_best else None)\n",
    "    return art\n",
    "\n",
    "\n",
    "def train_model(model, optimizer, scheduler, train_loader, val_loader, num_epochs=5):\n",
    "    \"A simple training loop\"\n",
    "    best_val_loss = 1e10\n",
    "    best_model_art = None\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            train_loss, _ = train_step(model, optimizer, scheduler, batch_x, batch_y)\n",
    "            wandb.log({\n",
    "                \"epoch\": epoch,\n",
    "                \"train_loss\": train_loss,\n",
    "                \"learning_rate\": optimizer.param_groups[0][\"lr\"]\n",
    "            })\n",
    "        val_loss, val_acc, _ = evaluate_model(model, val_loader)\n",
    "        wandb.log({\n",
    "            \"val_loss\": val_loss,\n",
    "            \"val_acc\": val_acc,\n",
    "        })\n",
    "        best_val_loss = min(best_val_loss, val_loss)\n",
    "        model_art = save_model(model, is_best=val_loss <= best_val_loss)\n",
    "        if val_loss <= best_val_loss:\n",
    "            best_model_art = model_art\n",
    "            print(\"New best model saved!\")\n",
    "        print(f\"Epoch {epoch}: val_loss: {val_loss}, val_acc: {val_acc}\")\n",
    "    return best_model_art "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Startup a W&B Run\n",
    "wandb.init(project=project_name, \n",
    "    job_type=\"model_trainer\",\n",
    "    config={\n",
    "        \"train_size\": 2_000,\n",
    "        \"val_size\": 200,\n",
    "        \"batch_size\": 64,\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"epochs\": 5,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Load in the training data\n",
    "config = wandb.config\n",
    "train_dl, val_dl = build_train_data(config.train_size, config.val_size, config.batch_size)\n",
    "\n",
    "# (Optional) Declare dataset dependency\n",
    "art = wandb.Artifact(f\"{dataset_name}-train\", \"dataset\")\n",
    "art.add_dir(\"./train_data\")\n",
    "wandb.use_artifact(art)\n",
    "\n",
    "# Define a model\n",
    "model, optimizer, scheduler = build_model(config.learning_rate, total_steps=len(train_dl) * config.epochs)\n",
    "  \n",
    "# Train the Model\n",
    "best_model_art = train_model(model, optimizer, scheduler, train_dl, val_dl, config.epochs)\n",
    "\n",
    "##### W&B MODEL MANAGEMENT SPECIFIC CALLS ######\n",
    "wandb.run.link_artifact(best_model_art, model_collection_name, [\"latest\"])\n",
    "\n",
    "# Finish the Run\n",
    "wandb.finish()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Use, Evaluate, and Promote a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import torch\n",
    "\n",
    "# Startup a W&B Run\n",
    "wandb.init(project=project_name, \n",
    "  job_type=\"model_evaluator\",\n",
    "  config={\n",
    "      \"test_size\": 100,\n",
    "  }\n",
    ")\n",
    "\n",
    "config = wandb.config\n",
    "\n",
    "# Load in the test data\n",
    "test_dl = build_test_data(test_size=config.test_size)\n",
    "\n",
    "# (Optional) Declare dataset dependency\n",
    "art = wandb.Artifact(f\"{dataset_name}-test\", \"dataset\")\n",
    "art.add_dir(\"./test_data\")\n",
    "wandb.use_artifact(art)\n",
    "\n",
    "##### W&B MODEL MANAGEMENT SPECIFIC CALLS ######\n",
    "model_art = wandb.use_artifact(f\"{model_collection_name}:latest\")\n",
    "model_path = model_art.get_path(\"model.pt\").download()\n",
    "\n",
    "# Load model from artifact\n",
    "model = Net().cpu()\n",
    "checkpt = torch.load(model_path)\n",
    "model.load_state_dict(checkpt)\n",
    "\n",
    "val_loss, val_acc, preds = evaluate_model(model, test_dl)\n",
    "\n",
    "table = wandb.Table(data=[], columns=[])\n",
    "table.add_column(\"image\", [wandb.Image(i.numpy()) for i in list(test_dl)[0][0]])\n",
    "table.add_column(\"label\", list(test_dl)[0][1].tolist())\n",
    "table.add_column(\"pred\", preds)\n",
    "\n",
    "wandb.log({\n",
    "    \"test_loss\": val_loss,\n",
    "    \"test_acc\": val_acc,\n",
    "    \"predictions\": table\n",
    "})\n",
    "\n",
    "##### W&B MODEL MANAGEMENT SPECIFIC CALLS ######\n",
    "wandb.run.link_artifact(model_art, model_collection_name, [\"latest\", \"production\"])\n",
    "\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
