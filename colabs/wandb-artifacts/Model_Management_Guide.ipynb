{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2b6af61",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/wandb/examples/blob/master/colabs/wandb-artifacts/Model_Management_Guide.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W&B Model Management Guide Companion Notebook\n",
    "\n",
    "**Please add `artifact-portfolios` to your bio in order to enable beta features required to complete this guide.**\n",
    "\n",
    "This is a comparnion notebook to the [W&B Model Management Guide](https://docs.wandb.ai/guides/models).\n",
    "\n",
    "**Table of Contents**\n",
    "* **Cell 1**: Installs `wandb` python library\n",
    "* **Cell 2** (Form): Allows you to specify some parameters and defines a handful of helper functions. Note: there is not any `wandb` specific library calls in these helper functions - they are purely used to allow the example cells to be more terse and focus on the key aspects of Model Management\n",
    "* **Cell 3**: (Train, Log, & Link Models) Covers steps [2. Traing & log a Model](https://docs.wandb.ai/guides/models#2.-train-and-log-model-versions) and [3. Link Model Versions to the Collection](https://docs.wandb.ai/guides/models#3.-link-model-versions-to-the-portfolio)\n",
    "* **Cell 4**: (Use, Evaluate, and Promote a Model) Covers steps [4. Using a Model Version](https://docs.wandb.ai/guides/models#4.-use-a-model-version), [5. Evaluate Model Performance](https://docs.wandb.ai/guides/models#5.-evaluate-model-performance), and [6. Promote a Version to Production](https://docs.wandb.ai/guides/models#6.-promote-a-version-to-production)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "**Stop! ðŸ›‘** Please complete [Step 1 of the tutorial](https://docs.wandb.ai/guides/models#1.-create-a-new-model-collection) before continuing. This will ensure you have a **Model Collection** defined in your project. Enter the Project name where you created the Collection in `project_name` and the name of the Collection in the `model_collection_name` fields respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wandb -U -qqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = \"model_management_docs_official_v0\" #@param {type:\"string\"}\n",
    "# dataset_name = \"mnist\" #@param {type:\"string\"}\n",
    "dataset_name = \"mnist\"\n",
    "model_collection_name = \"MNIST Grayscale 28x28\" #@param {type:\"string\"}\n",
    "use_beta_apis = False #@param {type:\"boolean\"}\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "def build_training_data(train_size, val_size, batch_size, data_path):\n",
    "  mnist_data = datasets.MNIST(\n",
    "      data_path, \n",
    "      download  = True, \n",
    "      transform = transforms.Compose([\n",
    "        transforms.ToTensor(), \n",
    "        ])\n",
    "      )\n",
    "\n",
    "  extra = 60000 - train_size - val_size\n",
    "  assert extra >= 0\n",
    "  splits = torch.utils.data.random_split(mnist_data, [train_size, val_size, extra])\n",
    "  return torch.utils.data.DataLoader(splits[0], batch_size=batch_size), torch.utils.data.DataLoader(splits[1], batch_size=val_size)\n",
    "\n",
    "def build_test_data(test_size, data_path):\n",
    "  mnist_data = datasets.MNIST(\n",
    "      data_path, \n",
    "      train = False,\n",
    "      download  = True, \n",
    "      transform = transforms.Compose([\n",
    "        transforms.ToTensor(), \n",
    "        ])\n",
    "      )\n",
    "\n",
    "  extra = 10000 - test_size\n",
    "  assert extra >= 0\n",
    "  splits = torch.utils.data.random_split(mnist_data, [test_size, extra])\n",
    "  return torch.utils.data.DataLoader(splits[0], batch_size=test_size)\n",
    "\n",
    "def build_model(learning_rate, gamma):\n",
    "  device = torch.device(\"cpu\")\n",
    "  model = Net().to(device)\n",
    "  optimizer = optim.Adadelta(model.parameters(), lr = learning_rate)\n",
    "  scheduler = StepLR(optimizer, step_size = 1, gamma = gamma)\n",
    "  return model, optimizer, scheduler\n",
    "\n",
    "def train_model_batch(model, optimizer, batch_x, batch_y):\n",
    "  model.train()\n",
    "  data, target = batch_x.to(\"cpu\"), batch_y.to(\"cpu\")\n",
    "  optimizer.zero_grad()\n",
    "  preds = model(data)\n",
    "  loss = F.nll_loss(preds, target)\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "  return loss.item(), preds\n",
    "\n",
    "def train_model_epoch(model, optimizer, train_loader, on_batch_end):\n",
    "  for batch_ndx, batch in enumerate(train_loader):\n",
    "    train_loss, preds = train_model_batch(model, optimizer, batch[0], batch[1])\n",
    "    on_batch_end(batch_ndx, batch, preds, train_loss)\n",
    "\n",
    "def train_model(model, optimizer, scheduler, train_loader, val_loader, num_epochs, on_batch_end, on_epoch_end):\n",
    "  for epoch_ndx in range(num_epochs):\n",
    "    def patched_on_batch_end(batch_ndx, batch, preds, train_loss):\n",
    "      on_batch_end(epoch_ndx, batch_ndx, batch, preds, train_loss)\n",
    "    train_model_epoch(model, optimizer, train_loader, patched_on_batch_end)\n",
    "    on_epoch_end(epoch_ndx)\n",
    "    scheduler.step()\n",
    "\n",
    "def evaluate_model(model, eval_data):\n",
    "    device = torch.device(\"cpu\")\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in eval_data:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(\n",
    "                output, target, reduction=\"sum\"\n",
    "            ).item()  # sum up batch loss\n",
    "            pred = output.argmax(\n",
    "                dim=1, keepdim=True\n",
    "            )  # get the index of the max log-probability\n",
    "            preds += list(pred.flatten().tolist())\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    test_loss /= len(eval_data.dataset)\n",
    "    accuracy = 100.0 * correct / len(eval_data.dataset)\n",
    "    return test_loss, accuracy, preds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train, Log, & Link Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from wandb.beta.workflows import log_model, link_model\n",
    "\n",
    "import torch\n",
    "import cloudpickle\n",
    "\n",
    "# Startup a W&B Run\n",
    "wandb.init(project=project_name, \n",
    "  job_type=\"model_trainer\",\n",
    "  config={\n",
    "      \"train_size\": 100,\n",
    "      \"val_size\": 30,\n",
    "      \"batch_size\": 64,\n",
    "      \"learning_rate\": 1.0,\n",
    "      \"gamma\": 0.75,\n",
    "      \"epochs\": 5,\n",
    "  }\n",
    ")\n",
    "\n",
    "# Load in the training data\n",
    "train_size = wandb.config[\"train_size\"]\n",
    "val_size = wandb.config[\"val_size\"]\n",
    "batch_size=wandb.config[\"batch_size\"]\n",
    "train_data_path = \"./train_data\"\n",
    "train, val = build_training_data(train_size, val_size, batch_size, train_data_path)\n",
    "\n",
    "# (Optional) Declare dataset dependency\n",
    "art = wandb.Artifact(f\"{dataset_name}-train\", \"dataset\")\n",
    "art.add_dir(train_data_path)\n",
    "wandb.use_artifact(art)\n",
    "\n",
    "# Define a model\n",
    "learning_rate = wandb.config[\"learning_rate\"]\n",
    "gamma = wandb.config[\"gamma\"]\n",
    "model, optimizer, scheduler = build_model(learning_rate, gamma)\n",
    "\n",
    "# Setup callbacks:\n",
    "def on_batch_end(epoch_ndx, batch_ndx, batch, preds, train_loss):\n",
    "  wandb.log({\n",
    "      \"epoch_ndx\": epoch_ndx,\n",
    "      \"batch_ndx\": batch_ndx,\n",
    "      \"train_loss\": train_loss,\n",
    "      \"learning_rate\": optimizer.param_groups[0][\"lr\"]\n",
    "  })\n",
    "\n",
    "best_loss = float(\"inf\")\n",
    "best_model = None\n",
    "def on_epoch_end(epoch_ndx):\n",
    "  global best_loss\n",
    "  global best_model\n",
    "  val_loss, val_acc, preds = evaluate_model(model, val)\n",
    "  is_best = val_loss < best_loss\n",
    "  if is_best:\n",
    "    best_loss = val_loss\n",
    "\n",
    "  ##### W&B MODEL MANAGEMENT SPEICIFIC CALLS ######\n",
    "  if use_beta_apis:\n",
    "    model_version = log_model(model, \"mnist\", [\"best\"] if is_best else None)\n",
    "    if is_best:\n",
    "      best_model = model_version\n",
    "  else:\n",
    "    art = wandb.Artifact(f\"mnist-{wandb.run.id}\", \"model\")\n",
    "    torch.save(model, \"model.pt\", pickle_module=cloudpickle)\n",
    "    art.add_file(\"model.pt\")\n",
    "    wandb.log_artifact(art, aliases=[\"best\", \"latest\"] if is_best else None)\n",
    "    if is_best:\n",
    "      best_model = art\n",
    "  wandb.log({\n",
    "      \"epoch_ndx\": epoch_ndx,\n",
    "      \"val_loss\": val_loss,\n",
    "      \"val_acc\": val_acc,\n",
    "      \"learning_rate\": optimizer.param_groups[0][\"lr\"],\n",
    "      \"best_loss\": best_loss\n",
    "  })\n",
    "  print(f\"Epoch {epoch_ndx}: val_loss: {val_loss}, val_acc: {val_acc}\")\n",
    "  \n",
    "\n",
    "# Train the Model\n",
    "epochs = wandb.config[\"epochs\"]\n",
    "train_model(model, optimizer, scheduler, train, val, epochs, on_batch_end, on_epoch_end)\n",
    "\n",
    "##### W&B MODEL MANAGEMENT SPEICIFIC CALLS ######\n",
    "if use_beta_apis:\n",
    "  link_model(best_model, model_collection_name)\n",
    "else:\n",
    "  wandb.run.link_artifact(best_model, model_collection_name, [\"latest\"])\n",
    "\n",
    "# Finish the Run\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use, Evaluate, and Promote a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from wandb.beta.workflows import use_model\n",
    "\n",
    "import torch\n",
    "# Startup a W&B Run\n",
    "wandb.init(project=project_name, \n",
    "  job_type=\"model_evaluator\",\n",
    "  config={\n",
    "      \"test_size\": 100,\n",
    "  }\n",
    ")\n",
    "\n",
    "# Load in the test data\n",
    "test_size = wandb.config[\"test_size\"]\n",
    "test_data_path = \"./test_data\"\n",
    "test = build_test_data(test_size, test_data_path)\n",
    "\n",
    "# (Optional) Declare dataset dependency\n",
    "art = wandb.Artifact(f\"{dataset_name}-test\", \"dataset\")\n",
    "art.add_dir(test_data_path)\n",
    "wandb.use_artifact(art)\n",
    "\n",
    "##### W&B MODEL MANAGEMENT SPEICIFIC CALLS ######\n",
    "if use_beta_apis:\n",
    "  model_art = use_model(f\"{model_collection_name}:latest\")\n",
    "  model_obj = model_art.model_obj()\n",
    "else:\n",
    "  model_art = wandb.use_artifact(f\"{model_collection_name}:latest\")\n",
    "  model_path = model_art.get_path(\"model.pt\").download()\n",
    "  model_obj = torch.load(model_path)\n",
    "\n",
    "model_obj.eval()\n",
    "val_loss, val_acc, preds = evaluate_model(model_obj, test)\n",
    "\n",
    "table = wandb.Table(data=[], columns=[])\n",
    "table.add_column(\"image\", [wandb.Image(i.numpy()) for i in list(test)[0][0]])\n",
    "table.add_column(\"label\", list(test)[0][1].tolist())\n",
    "table.add_column(\"pred\", preds)\n",
    "\n",
    "wandb.log({\n",
    "    \"test_loss\": val_loss,\n",
    "    \"test_acc\": val_acc,\n",
    "    \"predictions\": table\n",
    "})\n",
    "\n",
    "##### W&B MODEL MANAGEMENT SPEICIFIC CALLS ######\n",
    "if use_beta_apis:\n",
    "  link_model(model_art, model_collection_name, aliases=[\"production\"])\n",
    "else:\n",
    "  wandb.run.link_artifact(model_art, model_collection_name, [\"latest\", \"production\"])\n",
    "\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
