{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73de53e5-9e46-4edd-a64c-9778c3f7285e",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/wandb/examples/blob/master/colabs/mosaicml/MosaicML_Composer_and_wandb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "<!--- @wandbcode{mosaicml} -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910a2eb0-65b5-4b6c-971a-19bf8121ab13",
   "metadata": {},
   "source": [
    "<img src=\"https://wandb.me/logo-im-png\" width=\"400\" alt=\"Weights & Biases\" />\n",
    "<img src=\"https://raw.githubusercontent.com/mosaicml/composer/dev/docs/source/_static/images/header_dark.svg\" width=\"400\" alt=\"mosaicml\" />\n",
    "\n",
    "<!--- @wandbcode{mosaicml} -->\n",
    "\n",
    "# Running fast with MosaicML Composer and Weight and Biases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4961c393-0154-4937-98d7-16e472b9a0d3",
   "metadata": {},
   "source": [
    "[MosaicML Composer](https://docs.mosaicml.com) is a library for training neural networks better, faster, and cheaper. It contains many state-of-the-art methods for accelerating neural network training and improving generalization, along with an optional Trainer API that makes composing many different enhancements easy.\n",
    "\n",
    "Coupled with [Weights & Biases integration](https://docs.mosaicml.com/en/v0.5.0/trainer/logging.html), you can quickly train and monitor models for full traceability and reproducibility with only 2 extra lines of code:\n",
    "\n",
    "```python\n",
    "from composer.loggers import WandBLogger\n",
    "wandb_logger = WandBLogger()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec877f6-57aa-4423-ae4b-f85769c59dd6",
   "metadata": {},
   "source": [
    "W&B integration with Composer can automatically:\n",
    "* log your configuration parameters\n",
    "* log your losses and metrics\n",
    "* log gradients and parameter distributions\n",
    "* log your model\n",
    "* keep track of your code\n",
    "* log your system metrics (GPU, CPU, memory, temperature, etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092b0104-530a-438d-bd68-08f627cc8920",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 🛠️ Installation and set-up\n",
    "\n",
    "We need to install the following libraries:\n",
    "* [mosaicml-composer](https://docs.mosaicml.com/en/v0.5.0/getting_started/installation.html) to set up and train our models\n",
    "* [wandb](https://docs.wandb.ai/) to instrument our training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56bbcb21-babd-488b-a20d-080f43f09897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wandb in /home/paperspace/mambaforge/envs/data/lib/python3.9/site-packages (0.12.11)\n",
      "Collecting mosaicml\n",
      "  Downloading mosaicml-0.5.0-py3-none-any.whl (501 kB)\n",
      "\u001b[K     |████████████████████████████████| 501 kB 24.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: fastcore in /home/paperspace/mambaforge/envs/data/lib/python3.9/site-packages (1.3.29)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in /home/paperspace/mambaforge/envs/data/lib/python3.9/site-packages (from wandb) (8.0.4)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /home/paperspace/mambaforge/envs/data/lib/python3.9/site-packages (from wandb) (2.8.2)\n",
      "Requirement already satisfied: protobuf>=3.12.0 in /home/paperspace/mambaforge/envs/data/lib/python3.9/site-packages (from wandb) (3.19.4)\n",
      "Requirement already satisfied: GitPython>=1.0.0 in /home/paperspace/mambaforge/envs/data/lib/python3.9/site-packages (from wandb) (3.1.18)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /home/paperspace/mambaforge/envs/data/lib/python3.9/site-packages (from wandb) (1.5.7)\n",
      "Requirement already satisfied: PyYAML in /home/paperspace/mambaforge/envs/data/lib/python3.9/site-packages (from wandb) (6.0)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /home/paperspace/mambaforge/envs/data/lib/python3.9/site-packages (from wandb) (5.9.0)\n",
      "Requirement already satisfied: promise<3,>=2.0 in /home/paperspace/mambaforge/envs/data/lib/python3.9/site-packages (from wandb) (2.3)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /home/paperspace/mambaforge/envs/data/lib/python3.9/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /home/paperspace/mambaforge/envs/data/lib/python3.9/site-packages (from wandb) (2.27.1)\n",
      "Requirement already satisfied: shortuuid>=0.5.0 in /home/paperspace/mambaforge/envs/data/lib/python3.9/site-packages (from wandb) (1.0.8)\n",
      "Requirement already satisfied: yaspin>=1.0.0 in /home/paperspace/mambaforge/envs/data/lib/python3.9/site-packages (from wandb) (2.1.0)\n",
      "Requirement already satisfied: setproctitle in /home/paperspace/mambaforge/envs/data/lib/python3.9/site-packages (from wandb) (1.2.2)\n",
      "Requirement already satisfied: pathtools in /home/paperspace/mambaforge/envs/data/lib/python3.9/site-packages (from wandb) (0.1.2)\n",
      "Requirement already satisfied: six>=1.13.0 in /home/paperspace/mambaforge/envs/data/lib/python3.9/site-packages (from wandb) (1.16.0)\n",
      "Collecting numpy==1.21.5\n",
      "  Downloading numpy-1.21.5-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 15.7 MB 80.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: torchmetrics<0.8,>=0.7.0 in /home/paperspace/mambaforge/envs/data/lib/python3.9/site-packages (from mosaicml) (0.7.2)\n",
      "Collecting coolname<2,>=1.1.0\n",
      "  Downloading coolname-1.1.0-py2.py3-none-any.whl (35 kB)\n",
      "Requirement already satisfied: tqdm<5,>=4.62.3 in /home/paperspace/mambaforge/envs/data/lib/python3.9/site-packages (from mosaicml) (4.62.3)\n",
      "Collecting apache-libcloud<4,>=3.3.1\n",
      "  Downloading apache_libcloud-3.5.0-py2.py3-none-any.whl (4.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.0 MB 87.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: torchvision>=0.10.0 in /home/paperspace/mambaforge/envs/data/lib/python3.9/site-packages (from mosaicml) (0.12.0)\n",
      "Collecting torch-optimizer<0.2,>=0.1.0\n",
      "  Downloading torch_optimizer-0.1.0-py3-none-any.whl (72 kB)\n",
      "\u001b[K     |████████████████████████████████| 72 kB 2.1 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: torch<2,>=1.9 in /home/paperspace/mambaforge/envs/data/lib/python3.9/site-packages (from mosaicml) (1.11.0)\n",
      "Collecting PyYAML\n",
      "  Using cached PyYAML-5.4.1-cp39-cp39-manylinux1_x86_64.whl (630 kB)\n",
      "Collecting yahp==0.1.0\n",
      "  Downloading yahp-0.1.0-py3-none-any.whl (35 kB)\n",
      "Collecting ruamel.yaml>=0.17.10\n",
      "  Using cached ruamel.yaml-0.17.21-py3-none-any.whl (109 kB)\n",
      "Requirement already satisfied: pip in /home/paperspace/mambaforge/envs/data/lib/python3.9/site-packages (from fastcore) (21.2.4)\n",
      "Requirement already satisfied: packaging in /home/paperspace/mambaforge/envs/data/lib/python3.9/site-packages (from fastcore) (21.3)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/paperspace/mambaforge/envs/data/lib/python3.9/site-packages (from GitPython>=1.0.0->wandb) (4.0.7)\n",
      "Requirement already satisfied: smmap<5,>=3.0.1 in /home/paperspace/mambaforge/envs/data/lib/python3.9/site-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/paperspace/mambaforge/envs/data/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (1.26.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/paperspace/mambaforge/envs/data/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/paperspace/mambaforge/envs/data/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/paperspace/mambaforge/envs/data/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (3.3)\n",
      "Collecting ruamel.yaml.clib>=0.2.6\n",
      "  Using cached ruamel.yaml.clib-0.2.6-cp39-cp39-manylinux1_x86_64.whl (539 kB)\n",
      "Requirement already satisfied: typing_extensions in /home/paperspace/mambaforge/envs/data/lib/python3.9/site-packages (from torch<2,>=1.9->mosaicml) (3.10.0.2)\n",
      "Collecting pytorch-ranger>=0.1.1\n",
      "  Downloading pytorch_ranger-0.1.1-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: pyDeprecate==0.3.* in /home/paperspace/mambaforge/envs/data/lib/python3.9/site-packages (from torchmetrics<0.8,>=0.7.0->mosaicml) (0.3.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/paperspace/mambaforge/envs/data/lib/python3.9/site-packages (from torchvision>=0.10.0->mosaicml) (9.0.1)\n",
      "Requirement already satisfied: termcolor<2.0.0,>=1.1.0 in /home/paperspace/mambaforge/envs/data/lib/python3.9/site-packages (from yaspin>=1.0.0->wandb) (1.1.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/paperspace/mambaforge/envs/data/lib/python3.9/site-packages (from packaging->fastcore) (3.0.4)\n",
      "Installing collected packages: ruamel.yaml.clib, ruamel.yaml, PyYAML, pytorch-ranger, numpy, yahp, torch-optimizer, coolname, apache-libcloud, mosaicml\n",
      "  Attempting uninstall: PyYAML\n",
      "    Found existing installation: PyYAML 6.0\n",
      "    Uninstalling PyYAML-6.0:\n",
      "      Successfully uninstalled PyYAML-6.0\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.22.3\n",
      "    Uninstalling numpy-1.22.3:\n",
      "      Successfully uninstalled numpy-1.22.3\n",
      "Successfully installed PyYAML-5.4.1 apache-libcloud-3.5.0 coolname-1.1.0 mosaicml-0.5.0 numpy-1.21.5 pytorch-ranger-0.1.1 ruamel.yaml-0.17.21 ruamel.yaml.clib-0.2.6 torch-optimizer-0.1.0 yahp-0.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install wandb mosaicml fastcore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e54ec9-51b6-4f49-9ea1-2ed82f03add3",
   "metadata": {},
   "source": [
    "## Getting Started with Composer 🔥"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e0f570-d323-4ecf-bdb3-1469730f562b",
   "metadata": {},
   "source": [
    "Composer gives you access to a set of functions to speedup your models and infuse them with state of the art methods. For instance, you can insert [BlurPool](https://docs.mosaicml.com/en/latest/method_cards/blurpool.html) into your CNN by calling `CF.apply_blurpool(model)` into your PyTorch model. Take a look at all the [functional](https://docs.mosaicml.com/en/latest/functional_api.html) methods available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55d71a76-7354-43ee-87c1-a83aa88b0b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:composer.utils.module_surgery:optimizers was not provided. Be sure to either create the optimizer after\n",
      "invoking this method, or manually add new parameters to the existing optimizer.\n",
      "INFO:composer.algorithms.blurpool.blurpool:Applied BlurPool to model ResNet. Model now has 1 BlurMaxPool2d and 6 BlurConv2D layers.\n",
      "INFO:composer.utils.module_surgery:optimizers was not provided. Be sure to either create the optimizer after\n",
      "invoking this method, or manually add new parameters to the existing optimizer.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from composer import functional as CF\n",
    "import torchvision.models as models\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "model = models.resnet50()\n",
    "\n",
    "# replace some layers with blurpool\n",
    "CF.apply_blurpool(model);\n",
    "# replace some layers with squeeze-excite\n",
    "CF.apply_squeeze_excite(model, latent_channels=64, min_channels=128);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ad4bdf-9d1f-4bf6-8e30-ff72e06c6718",
   "metadata": {},
   "source": [
    "> 💡 you can use this upgraded model with your favourite PyTorch training or... "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642eb0d5-b4f6-4add-9d59-235222bc2236",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Use the `Trainer` class with Weights and Biases 🏋️‍♀️\n",
    "\n",
    "W&B integration with MosaicML-Composer is built into the `Trainer` and can be configured to add extra functionalities through `WandBLogger`:\n",
    "\n",
    "* logging of Artifacts: Use `log_artifacts=True` to log model checkpoints as `wandb.Artifacts`. You can setup how often by passing an int value to `log_artifacts_every_n_batches` (default = 100)\n",
    "* you can also pass any parameter that you would pass to `wandb.init` in `init_params` as a dictionary. For example, you could pass `init_params = {\"project\":\"try_mosaicml\", \"name\":\"benchmark\", \"entity\":\"user_name\"}`.\n",
    "\n",
    "For more details refer to [Logger documentation](https://docs.mosaicml.com/en/latest/api_reference/composer.loggers.wandb_logger.html#composer.loggers.wandb_logger.WandBLogger) and [Wandb docs](https://docs.wandb.ai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "caecb6eb-25b9-4daf-a1d2-465fecf98ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 5\n",
    "BS = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93fbba13-dbb9-436e-8237-fa60f0e54675",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from composer import Callback, State, Logger, Trainer\n",
    "from composer.models import MNIST_Classifier\n",
    "from composer.loggers import WandBLogger, TQDMLogger\n",
    "from composer.callbacks import SpeedMonitor, LRMonitor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f42d23-1e98-46b7-872d-17e326a30ef1",
   "metadata": {},
   "source": [
    "let's grab a copy of MNIST from `torchvision`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5df9ec80-22ec-4cad-91a3-685a1392dcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.MNIST(\".\", train=True, download=True, transform=T.ToTensor())\n",
    "eval_dataset  = datasets.MNIST(\".\", train=False, download=True, transform=T.ToTensor())\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=BS, num_workers=2)\n",
    "eval_dataloader  = DataLoader(eval_dataset, batch_size=2*BS, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13640e3-7ff3-4fb2-a795-6443568cbf3e",
   "metadata": {},
   "source": [
    "we can import a simple ConvNet model to try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb8f7103-cad2-4f40-9d48-5ac2b394b4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MNIST_Classifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa820b1-12bb-43c8-ae7c-5cea24b69c35",
   "metadata": {},
   "source": [
    "### 📊 Tracking the experiment\n",
    "> we define the `wandb.init` params here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c4424591-cd28-4af2-987b-be0588a99a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config params to log\n",
    "config = {\"epochs\":EPOCHS, \n",
    "          \"batch_size\":BS,\n",
    "          \"model_name\":\"MNIST_Classifier\"}\n",
    "\n",
    "# these will get passed to wandb.init(**init_params)\n",
    "init_params = {\"project\":\"mosaic_ml\", \n",
    "               \"name\":\"mnist_baseline\",\n",
    "               \"config\":config}\n",
    "\n",
    "# setup of the logger \n",
    "wandb_logger = WandBLogger(init_params=init_params)\n",
    "\n",
    "# we also add progressbar logging\n",
    "progress_logger = TQDMLogger()\n",
    "\n",
    "loggers = [progress_logger, wandb_logger]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2b5740-7736-4f6d-88f1-c89a669691fe",
   "metadata": {},
   "source": [
    "we are able to tweak what are we logging using `Callbacks` into the `Trainer` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e60c66bc-f046-4a57-9ea7-a6e9fe5cfc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [LRMonitor(),    # Logs the learning rate\n",
    "             SpeedMonitor(), # Logs the training throughput\n",
    "            ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e647bcc-abe5-45cd-bb7d-fa91e6c2006e",
   "metadata": {},
   "source": [
    "we include callbacks that measure the model throughput (and the learning rate) and logs them to Weights & Biases. [Callbacks](https://docs.mosaicml.com/en/latest/trainer/callbacks.html) control what is being logged, whereas loggers specify where the information is being saved. For more information on loggers, see [Logging](https://docs.mosaicml.com/en/latest/trainer/logging.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fb0ad37f-943a-4341-8a16-32bf88e98ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:composer.trainer.trainer:Setting seed to 3538293801\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/paperspace/wandb/examples/colabs/mosaicml/wandb/run-20220323_122846-1k3sdkst</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/capecape/mosaic_ml/runs/1k3sdkst\" target=\"_blank\">mnist_baseline</a></strong> to <a href=\"https://wandb.ai/capecape/mosaic_ml\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataloader=train_dataloader,\n",
    "    eval_dataloader=eval_dataloader,\n",
    "    max_duration=f\"{EPOCHS}ep\",\n",
    "    loggers=loggers,\n",
    "    callbacks=callbacks,\n",
    "    device=\"gpu\",     # to train on GPU\n",
    "    precision=\"amp\",  # use mixed precision training, nice speed bump\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae01513-21a8-49a5-bec3-83bebf079710",
   "metadata": {},
   "source": [
    "once we are ready to train we call `fit`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "99044ec7-3e07-4c21-8ba2-9eed79ecb801",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bbb740ea4c74dbb95bb847dace609d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 0:   0%|          | 0/1875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47b4e6bcdbc448d9bcb1cc8be6ee6cf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1, Batch 1875 (val):   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "138f8b6ad1f14267ae1699f6a444cf6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/1875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63e24a9fbdfc4ceeb478e01d9149a148",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2, Batch 3750 (val):   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e42f31861a92496fa4ff6263a2091f31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2:   0%|          | 0/1875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e1c7de9b33a4facae9445a6a1d2e4d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3, Batch 5625 (val):   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39d07d1c0c734fbc90ac125d7aeeb350",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3:   0%|          | 0/1875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df317344c50a4713a129a9bcb44395b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4, Batch 7500 (val):   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88ea850680f140c29ab17c5c68758c47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4:   0%|          | 0/1875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3667cc8d6f724a76bb4bdcf4dcff5bc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5, Batch 9375 (val):   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy/val</td><td>▁▄▄▇█</td></tr><tr><td>crossentropyloss/val</td><td>█▅▄▂▁</td></tr><tr><td>epoch</td><td>▁▂▄▅▇█</td></tr><tr><td>loss/train</td><td>█▅▃▄▂▃▅▃▂▂▁▂▁▁▁▅▁▂▁▂▁▂▅▅▁█▅▁▁▂▁▃▁▃▂▃▁▁▁▄</td></tr><tr><td>lr-DecoupledSGDW/group0</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>throughput/epoch</td><td>▁▆▇██</td></tr><tr><td>throughput/step</td><td>▆▁▆▆▅▆▆▆▆▁▇▆▅▇▆▅▇▂▅▇▇▆▆▇█▇▇▇▇▇█▆▇▆▇▇▇▇▇▅</td></tr><tr><td>trainer/batch_idx</td><td>▁▂▃▄▅▆▇▇▁▂▃▄▅▆▆▇▁▂▃▄▅▆▇█▁▂▃▄▅▆▇█▂▂▃▄▅▆▇█</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>wall_clock_train</td><td>▁▃▅▆█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy/val</td><td>0.9894</td></tr><tr><td>crossentropyloss/val</td><td>0.03542</td></tr><tr><td>epoch</td><td>5</td></tr><tr><td>loss/train</td><td>0.00197</td></tr><tr><td>lr-DecoupledSGDW/group0</td><td>0.1</td></tr><tr><td>throughput/epoch</td><td>3974.03641</td></tr><tr><td>throughput/step</td><td>3807.48171</td></tr><tr><td>trainer/batch_idx</td><td>1874</td></tr><tr><td>trainer/global_step</td><td>9375</td></tr><tr><td>wall_clock_train</td><td>77.9812</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">mnist_baseline</strong>: <a href=\"https://wandb.ai/capecape/mosaic_ml/runs/1k3sdkst\" target=\"_blank\">https://wandb.ai/capecape/mosaic_ml/runs/1k3sdkst</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220323_122846-1k3sdkst/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22563587-a1bc-4885-95a7-81d47858efe4",
   "metadata": {},
   "source": [
    "## ⚙️ Advanced: Using callbacks to log sample predictions\n",
    "\n",
    "> Composer is extensible through its callback system.\n",
    "\n",
    "We create a custom callback to automatically log sample predictions during validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b4c0116c-70e0-4f34-828a-1c911c709346",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogPredictions(Callback):\n",
    "    def __init__(self, num_samples=100):\n",
    "        super().__init__()\n",
    "        self.num_samples = num_samples\n",
    "        self.data = []\n",
    "        \n",
    "    def eval_batch_end(self, state: State, logger: Logger):\n",
    "        \"\"\"Compute predictions per batch and stores them on self.data\"\"\"\n",
    "        if state.timer.epoch == state.max_duration: # on last val epoch\n",
    "            if len(self.data) < self.num_samples:\n",
    "                n = self.num_samples\n",
    "                x, y = state.batch_pair\n",
    "                outputs = state.outputs.argmax(-1)\n",
    "                data = [[wandb.Image(x_i), y_i, y_pred] for x_i, y_i, y_pred in list(zip(x[:n], y[:n], outputs[:n]))]\n",
    "                self.data += data\n",
    "            \n",
    "    def eval_end(self, state: State, logger: Logger):\n",
    "        \"Create a wandb.Table and logs it\"\n",
    "        columns = ['image', 'ground truth', 'prediction']\n",
    "        table = wandb.Table(columns=columns, data=self.data[:self.num_samples])\n",
    "        wandb.log({'predictions_table':table}, step=int(state.timer.batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e816d1-a114-4bca-8a7b-a70b13969366",
   "metadata": {},
   "source": [
    "we add `LogPredictions` to the other callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "682b52b0-5055-4cfd-a6c1-1a114390e965",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks.append(LogPredictions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cd7e36-e0c9-4235-980f-f590500478a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:composer.trainer.trainer:Setting seed to 413607077\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/paperspace/wandb/examples/colabs/mosaicml/wandb/run-20220323_123210-3ejku9sx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/capecape/mosaic_ml/runs/3ejku9sx\" target=\"_blank\">mnist_baseline</a></strong> to <a href=\"https://wandb.ai/capecape/mosaic_ml\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataloader=train_dataloader,\n",
    "    eval_dataloader=eval_dataloader,\n",
    "    max_duration=f\"{EPOCHS}ep\",\n",
    "    loggers=loggers,\n",
    "    callbacks=callbacks,\n",
    "    device=\"gpu\",     # to train on GPU\n",
    "    precision=\"amp\",  # use mixed precision training, nice speed bump\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea27598-8d08-4ce0-b3ef-ed34fd161e48",
   "metadata": {},
   "source": [
    "Once we're ready to train, we just call the `fit` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6ccc77ed-bfd3-4698-a9a4-431b8a46079d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7141dad83a6e40bcaed403c4c1373f5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 0:   0%|          | 0/1875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c205b2d3664f46ccbfb552e4e9ed57d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1, Batch 1875 (val):   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be0239f252344e3583080e5935c449ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/1875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fd0df0cf999429c9e2c43397ca433af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2, Batch 3750 (val):   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1322dff851614cb59fab531940d9aa2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2:   0%|          | 0/1875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bec531ec658498baeace73dc350616d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3, Batch 5625 (val):   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8b804cd33e641a09c0cdc3acb8f5896",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3:   0%|          | 0/1875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bdcb9e2251e4b66848dc22f37c6e18d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4, Batch 7500 (val):   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9afef5cd31747fab22ba43eac027868",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4:   0%|          | 0/1875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6119dc4e9c18469190b80ae896d9d081",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5, Batch 9375 (val):   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.066 MB of 0.067 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.975071…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy/val</td><td>▄▁▇█▅</td></tr><tr><td>crossentropyloss/val</td><td>▅██▁▇</td></tr><tr><td>epoch</td><td>▁▂▄▅▇█</td></tr><tr><td>loss/train</td><td>▄▁▂▆▆▁▁▂▁▄▁▂▅▂▁▁▃▁▂▁▁▁▄█▃▂▁▁▁▂▁▂▂▁▁▂▂▂▄▁</td></tr><tr><td>lr-DecoupledSGDW/group0</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>throughput/epoch</td><td>█▁▅▇▅</td></tr><tr><td>throughput/step</td><td>▃▁▂▄▃▃▆▆▄▃▅▂▆▁▁▂▇▁▄▃▁▁▃█▄▃▃▅▅▆▂▄▆▁▄▅▃▅▂▃</td></tr><tr><td>trainer/batch_idx</td><td>▁▂▃▄▅▆▇▇▁▂▃▄▅▆▆▇▁▂▃▄▅▆▇█▁▂▃▄▅▆▇█▂▂▃▄▅▆▇█</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>wall_clock_train</td><td>▁▃▅▆█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy/val</td><td>0.9897</td></tr><tr><td>crossentropyloss/val</td><td>0.03177</td></tr><tr><td>epoch</td><td>5</td></tr><tr><td>loss/train</td><td>0.00249</td></tr><tr><td>lr-DecoupledSGDW/group0</td><td>0.1</td></tr><tr><td>throughput/epoch</td><td>3903.67549</td></tr><tr><td>throughput/step</td><td>3766.30361</td></tr><tr><td>trainer/batch_idx</td><td>1874</td></tr><tr><td>trainer/global_step</td><td>9375</td></tr><tr><td>wall_clock_train</td><td>154.77541</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">mnist_baseline</strong>: <a href=\"https://wandb.ai/capecape/mosaic_ml/runs/3ejku9sx\" target=\"_blank\">https://wandb.ai/capecape/mosaic_ml/runs/3ejku9sx</a><br/>Synced 5 W&B file(s), 5 media file(s), 102 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220323_123210-3ejku9sx/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfd5172-59b2-4895-97e9-8e8906ee7147",
   "metadata": {},
   "source": [
    "We can monitor losses, metrics, gradients, parameters and sample predictions as the model trains."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74c0eba-58ce-435a-8bee-9c55663e17de",
   "metadata": {},
   "source": [
    "![composer.png](https://i.imgur.com/VFZLOB3.png?1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0ddffd-2ed4-4be6-848e-d95e373cc84d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 📚 Resources\n",
    "\n",
    "* We are excited to showcase this early support of [MosaicML-Composer](https://docs.mosaicml.com/en/latest/index.html) go ahead and try this new state of the art framework."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c60f734-8eef-4b95-b4e1-d4d33ff48399",
   "metadata": {},
   "source": [
    "## ❓ Questions about W&B\n",
    "\n",
    "If you have any questions about using W&B to track your model performance and predictions, please reach out to the [wandb community](https://community.wandb.ai)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt",
   "language": "python",
   "name": "pt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
