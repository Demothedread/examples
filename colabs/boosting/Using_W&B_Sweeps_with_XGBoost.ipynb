{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " Using W&B Sweeps with XGBoost",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3IN6tFfRD1p"
      },
      "source": [
        "# Introduction to Hyperparameter Sweeps using W&B\n",
        "\n",
        "Searching through high dimensional hyperparameter spaces to find the most performant model can get unwieldy very fast. Hyperparameter sweeps provide an organized and efficient way to conduct a battle royale of models and pick the most accurate model. They enable this by automatically searching through combinations of hyperparameter values (e.g. learning rate, batch size, number of hidden layers, optimizer type) to find the most optimal values.\n",
        "\n",
        "In this tutorial we'll see how you can run sophisticated hyperparameter sweeps in 3 easy steps using Weights and Biases.\n",
        "\n",
        "![](https://i.imgur.com/WVKkMWw.png)\n",
        "\n",
        "## Sweeps: An Overview\n",
        "\n",
        "Running a hyperparameter sweep with Weights & Biases is very easy. There are just 3 simple steps:\n",
        "\n",
        "1. **Define the sweep:** we do this by creating a dictionary or a [YAML file](https://docs.wandb.com/library/sweeps/configuration) that specifies the parameters to search through, the search strategy, the optimization metric et all.\n",
        "\n",
        "2. **Initialize the sweep:** with one line of code we initialize the sweep and pass in the dictionary of sweep configurations:\n",
        "`sweep_id = wandb.sweep(sweep_config)`\n",
        "\n",
        "3. **Run the sweep agent:** also accomplished with one line of code, we call wandb.agent() and pass the sweep_id to run, along with a function that defines your model architecture and trains it:\n",
        "`wandb.agent(sweep_id, function=train)`\n",
        "\n",
        "And voila! That's all there is to running a hyperparameter sweep! In the notebook below, we'll walk through these 3 steps in more detail.\n",
        "\n",
        "\n",
        "We highly encourage you to fork this notebook, tweak the parameters, or try the model with your own dataset!\n",
        "\n",
        "## Resources\n",
        "- [Sweeps docs →](https://docs.wandb.com/library/sweeps)\n",
        "- [Launching from the command line →](https://www.wandb.com/articles/hyperparameter-tuning-as-easy-as-1-2-3)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SiborLNTviW"
      },
      "source": [
        "!pip install wandb\n",
        "!wandb login"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8NZSgFqiJz9"
      },
      "source": [
        "## 1. Define the Sweep\n",
        "\n",
        "Weights & Biases sweeps give you powerful levers to configure your sweeps exactly how you want them, with just a few lines of code. The sweeps config can be defined as a dictionary or a [YAML file](https://docs.wandb.com/library/sweeps).\n",
        "\n",
        "Let's walk through some of them together:\n",
        "*   **Metric** – This is the metric the sweeps are attempting to optimize. Metrics can take a `name` (this metric should be logged by your training script) and a `goal` (maximize or minimize). \n",
        "*   **Search Strategy** – Specified using the 'method' variable. We support several different search strategies with sweeps. \n",
        "  *   **Grid Search** – Iterates over every combination of hyperparameter values.\n",
        "  *   **Random Search** – Iterates over randomly chosen combinations of hyperparameter values.\n",
        "  *   **Bayesian Search** – Creates a probabilistic model that maps hyperparameters to probability of a metric score, and chooses parameters with high probability of improving the metric. The objective of Bayesian optimization is to spend more time in picking the hyperparameter values, but in doing so trying out fewer hyperparameter values.\n",
        "*   **Stopping Criteria** – The strategy for determining when to kill off poorly peforming runs, and try more combinations faster. We offer several custom scheduling algorithms like [HyperBand](https://arxiv.org/pdf/1603.06560.pdf) and Envelope.\n",
        "*   **Parameters** – A dictionary containing the hyperparameter names, and discreet values, max and min values or distributions from which to pull their values to sweep over.\n",
        "\n",
        "You can find a list of all configuration options [here](https://docs.wandb.com/library/sweeps/configuration)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdUNayB2Q1L1"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHw_OmGFUC4G"
      },
      "source": [
        "import wandb\n",
        "sweep_config = {\n",
        "    'method': 'random', #grid, random\n",
        "    'metric': {\n",
        "      'name': 'accuracy',\n",
        "      'goal': 'maximize'   \n",
        "    },\n",
        "    'parameters': {\n",
        "        'booster': {\n",
        "            'values': ['gbtree','gblinear']\n",
        "        },\n",
        "        'max_depth': {\n",
        "            'values': [3, 6, 9, 12]\n",
        "        },\n",
        "        'learning_rate': {\n",
        "            'values': [0.1, 0.05, 0.2]\n",
        "        },\n",
        "        'subsample': {\n",
        "            'values': [1, 0.5, 0.3]\n",
        "        }\n",
        "    }\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQc9kV1PiTLi"
      },
      "source": [
        "## 2. Initialize the Sweep"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJ7BdPM3VVKx"
      },
      "source": [
        "sweep_id = wandb.sweep(sweep_config, project=\"XGboost-sweeps\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ww4_-CDYiZ8f"
      },
      "source": [
        "### Define Your Model\n",
        "Before we can run the sweep, let's define a function that creates and trains our model.\n",
        "\n",
        "*   **wandb.init()** – Initialize a new W&B run. Each run is single execution of the training script.\n",
        "*   **wandb.config** – Save all your hyperparameters in a config object. This lets you use our app to sort and compare your runs by hyperparameter values.\n",
        "*   **wandb.log()** – Logs custom objects – these can be images, videos, audio files, HTML, plots, point clouds etc. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Lqp6cOFR2cS"
      },
      "source": [
        "\n",
        "# First XGBoost model for Pima Indians dataset\n",
        "from numpy import loadtxt\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "# load data\n",
        "def train():\n",
        "  config_defaults = {\n",
        "        'booster': 'gbtree',\n",
        "        'max_depth': 3,\n",
        "        'learning_rate': 0.1,\n",
        "        'subsample': 1\n",
        "  }\n",
        "  wandb.init(config=config_defaults)\n",
        "  config = wandb.config\n",
        "  dataset = loadtxt('pima-indians-diabetes.data.csv', delimiter=\",\")\n",
        "  # split data into X and y\n",
        "  X = dataset[:,0:8]\n",
        "  Y = dataset[:,8]\n",
        "  # split data into train and test sets\n",
        "  seed = 7\n",
        "  test_size = 0.33\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=test_size, random_state=seed)\n",
        "  # fit model no training data\n",
        "  model = XGBClassifier(booster = config.booster,max_depth= config.max_depth, learning_rate = config.learning_rate, \n",
        "                        subsample = config.subsample)\n",
        "  model.fit(X_train, y_train)\n",
        "  # make predictions for test data\n",
        "  y_pred = model.predict(X_test)\n",
        "  predictions = [round(value) for value in y_pred]\n",
        "  # evaluate predictions\n",
        "  accuracy = accuracy_score(y_test, predictions)\n",
        "  print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
        "  wandb.log({\"accuracy\":accuracy*100.0})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmmVE5YTilK-"
      },
      "source": [
        "## 3. Run the sweep agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "coDcjoF_SlMP"
      },
      "source": [
        "wandb.agent(sweep_id, train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gexIMFncij55"
      },
      "source": [
        "## Visualize Sweeps Results\n",
        "\n",
        "### Parallel coordinates plot\n",
        "This plot maps hyperparameter values to model metrics. It’s useful for honing in on combinations of hyperparameters that led to the best model performance.\n",
        "\n",
        "![](https://assets.website-files.com/5ac6b7f2924c652fd013a891/5e190366778ad831455f9af2_s_194708415DEC35F74A7691FF6810D3B14703D1EFE1672ED29000BA98171242A5_1578695138341_image.png)\n",
        "\n",
        "### Hyperparameter Importance Plot\n",
        "The hyperparameter importance plot surfaces which hyperparameters were the best predictors of, and highly correlated to desirable values for your metrics.\n",
        "\n",
        "![](https://assets.website-files.com/5ac6b7f2924c652fd013a891/5e190367778ad820b35f9af5_s_194708415DEC35F74A7691FF6810D3B14703D1EFE1672ED29000BA98171242A5_1578695757573_image.png)\n",
        "\n",
        "These visualizations can help you save both time and resources running expensive hyperparameter optimizations by honing in on the parameters (and value ranges) that are the most important, and thereby worthy of further exploration.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUIlNc2nNN0c"
      },
      "source": [
        "\n",
        "# Next steps - Get your hands dirty with sweeps\n",
        "\n",
        "We created a simple training script and [a few flavors of sweep configs](https://github.com/wandb/examples/tree/master/keras-cnn-fashion) for you to play with. We highly encourage you to give these a try. This repo also has examples to help you try more advanced sweep features like [Bayesian Hyperband](https://app.wandb.ai/wandb/examples-keras-cnn-fashion/sweeps/us0ifmrf?workspace=user-lavanyashukla), and [Hyperopt](https://app.wandb.ai/wandb/examples-keras-cnn-fashion/sweeps/xbs2wm5e?workspace=user-lavanyashukla)."
      ]
    }
  ]
}