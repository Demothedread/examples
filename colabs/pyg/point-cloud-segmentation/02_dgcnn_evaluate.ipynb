{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9385afd2-4e70-4fc3-820b-576bdf035e42",
   "metadata": {},
   "source": [
    "# üî•üî• Evaluate DGCNN Model Weights & Biases ü™Ñüêù\n",
    "\n",
    "<!--- @wandbcode{pyg-dgcnn-eval} -->\n",
    "\n",
    "This notebook demonstrates the evaluation of [Dynamic Graph CNN](https://arxiv.org/pdf/1801.07829.pdf) for point cloud segmnetation. You can check the following notebook for referring to the training code:\n",
    "\n",
    "[![](https://colab.research.google.com/assets/colab-badge.svg)]()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e79b763-e197-4a1d-9f3f-f89d873c12ed",
   "metadata": {},
   "source": [
    "# Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e46eac0-9bf8-4c3b-906a-ca56343810ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "os.environ['TORCH'] = torch.__version__\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a87ef2-abf9-4d71-a4ae-d58e3ebba706",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "!pip install -q torch-cluster -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n",
    "!pip install -q wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267f5a6e-fedd-48ef-9861-66e8ffb9fbbd",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d8cf4d8-6839-47fb-b789-857997e15107",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wandb\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch_scatter import scatter\n",
    "from torchmetrics.functional import jaccard_index\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import ShapeNet\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import MLP, DynamicEdgeConv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0643258d-84be-4a10-a67e-0cf0331e07aa",
   "metadata": {},
   "source": [
    "# Initialize Weights & Biases\n",
    "\n",
    "We need to call [`wandb.init()`](https://docs.wandb.ai/ref/python/init) once at the beginning of our program to initialize a new job. This creates a new run in W&B and launches a background process to sync data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "308fec6f-7397-4c9d-a83d-c7b5632053b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgeekyrakshit\u001b[0m (\u001b[33mwandb\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jupyter/segmentation/wandb/run-20221220_110159-m90yxuty</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/wandb/point-cloud-segmentation/runs/m90yxuty\" target=\"_blank\">dgcnn</a></strong> to <a href=\"https://wandb.ai/wandb/point-cloud-segmentation\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb_project = \"pyg-point-cloud\" #@param {\"type\": \"string\"}\n",
    "wandb_run_name = \"evaluate-dgcnn\" #@param {\"type\": \"string\"}\n",
    "\n",
    "wandb.init(project=wandb_project, name=wandb_run_name, job_type=\"evaluate\")\n",
    "\n",
    "config = wandb.config\n",
    "\n",
    "config.seed = 42\n",
    "config.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "random.seed(config.seed)\n",
    "torch.manual_seed(config.seed)\n",
    "device = torch.device(config.device)\n",
    "\n",
    "config.category = 'Airplane' #@param [\"Bag\", \"Cap\", \"Car\", \"Chair\", \"Earphone\", \"Guitar\", \"Knife\", \"Lamp\", \"Laptop\", \"Motorbike\", \"Mug\", \"Pistol\", \"Rocket\", \"Skateboard\", \"Table\"] {type:\"raw\"}\n",
    "config.random_jitter_translation = 1e-2\n",
    "config.random_rotation_interval_x = 15\n",
    "config.random_rotation_interval_y = 15\n",
    "config.random_rotation_interval_z = 15\n",
    "config.batch_size = 1\n",
    "config.num_workers = 6\n",
    "\n",
    "config.num_nearest_neighbours = 30\n",
    "config.aggregation_operator = \"max\"\n",
    "config.dropout = 0.5\n",
    "config.initial_lr = 1e-3\n",
    "config.lr_scheduler_step_size = 20\n",
    "config.gamma = 0.8\n",
    "\n",
    "config.artifact_address = 'wandb/point-cloud-segmentation/dgcnn-3n97rfrv-checkpoint:v29'\n",
    "config.epochs = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f068eb-5b69-4c73-9e21-4f0d5eab0d9e",
   "metadata": {},
   "source": [
    "# Load ShapeNet Dataset using PyTorch Geometric\n",
    "\n",
    "We now load, preprocess and batch the ModelNet dataset for training, validation/testing and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "208dc676-c0bf-4a3d-bc5a-5b4251664de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = T.Compose([\n",
    "    T.RandomJitter(config.random_jitter_translation),\n",
    "    T.RandomRotate(config.random_rotation_interval_x, axis=0),\n",
    "    T.RandomRotate(config.random_rotation_interval_y, axis=1),\n",
    "    T.RandomRotate(config.random_rotation_interval_z, axis=2)\n",
    "])\n",
    "pre_transform = T.NormalizeScale()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27014b34-cfff-47be-b761-db8c484dad97",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = os.path.join('ShapeNet', config.category)\n",
    "\n",
    "train_dataset = ShapeNet(\n",
    "    dataset_path, config.category, split='trainval',\n",
    "    transform=transform, pre_transform=pre_transform\n",
    ")\n",
    "test_dataset = ShapeNet(\n",
    "    dataset_path, config.category, split='test',\n",
    "    pre_transform=pre_transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8d0e1df-7522-485b-b697-3dd066d85f8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1398e0453f734627b3c64fc8e6c25717",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2349 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4302f63a16d42f5be17be79db532a37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/341 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segmentation_class_frequency = {}\n",
    "for idx in tqdm(range(len(train_dataset))):\n",
    "    pc_viz = train_dataset[idx].pos.numpy().tolist()\n",
    "    segmentation_label = train_dataset[idx].y.numpy().tolist()\n",
    "    for label in set(segmentation_label):\n",
    "        segmentation_class_frequency[label] = segmentation_label.count(label)\n",
    "for idx in tqdm(range(len(test_dataset))):\n",
    "    pc_viz = train_dataset[idx].pos.numpy().tolist()\n",
    "    segmentation_label = train_dataset[idx].y.numpy().tolist()\n",
    "    for label in set(segmentation_label):\n",
    "        segmentation_class_frequency[label] = segmentation_label.count(label)\n",
    "class_offset = min(list(segmentation_class_frequency.keys()))\n",
    "class_offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c67c4fa-92b4-495a-a247-c4e8a6c56d79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8737684cdc55450e9fc8c51f52f29d8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2349 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd231595aba748549e422e53c269684a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/341 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for idx in tqdm(range(len(train_dataset))):\n",
    "    train_dataset[idx].y -= class_offset\n",
    "for idx in tqdm(range(len(test_dataset))):\n",
    "    test_dataset[idx].y -= class_offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5147d1c3-9772-4be9-8932-c9238a9270ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=config.batch_size,\n",
    "    shuffle=True, num_workers=config.num_workers\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size=config.batch_size,\n",
    "    shuffle=False, num_workers=config.num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce1b72e-cd7b-452f-ac08-dc2155561077",
   "metadata": {},
   "source": [
    "# Load Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8a02d75-be84-436a-871c-ccadb863dc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DGCNN(torch.nn.Module):\n",
    "    def __init__(self, out_channels, k=30, aggr='max'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = DynamicEdgeConv(MLP([2 * 6, 64, 64]), k, aggr)\n",
    "        self.conv2 = DynamicEdgeConv(MLP([2 * 64, 64, 64]), k, aggr)\n",
    "        self.conv3 = DynamicEdgeConv(MLP([2 * 64, 64, 64]), k, aggr)\n",
    "\n",
    "        self.mlp = MLP(\n",
    "            [3 * 64, 1024, 256, 128, out_channels],\n",
    "            dropout=0.5, norm=None\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, pos, batch = data.x, data.pos, data.batch\n",
    "        x0 = torch.cat([x, pos], dim=-1)\n",
    "        \n",
    "        x1 = self.conv1(x0, batch)\n",
    "        x2 = self.conv2(x1, batch)\n",
    "        x3 = self.conv3(x2, batch)\n",
    "        \n",
    "        out = self.mlp(torch.cat([x1, x2, x3], dim=1))\n",
    "        return F.log_softmax(out, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c1f670-2a87-4083-9ec7-ed07785a5d66",
   "metadata": {},
   "source": [
    "Since we saved the checkpoints as artifacts on our Weights & Biases workspace, we can now fetch and load them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22cb064c-58b4-4887-b31b-d16622405b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.num_classes = train_dataset.num_classes\n",
    "\n",
    "model = DGCNN(\n",
    "    out_channels=train_dataset.num_classes,\n",
    "    k=config.num_nearest_neighbours,\n",
    "    aggr=config.aggregation_operator\n",
    ").to(device)\n",
    "\n",
    "model_artifact = wandb.use_artifact(config.artifact_address, type='checkpoint')\n",
    "artifact_dir = model_artifact.download()\n",
    "model_checkpoint_path = os.path.join(artifact_dir, \"checkpoint.pt\")\n",
    "\n",
    "model.load_state_dict(torch.load(model_checkpoint_path)[\"model_state_dict\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a676bbf9-50be-4a99-9222-427c927bbe57",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98e79961-e71c-4a2f-9040-72c6a5b32f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(loader, split, table):\n",
    "    total_accuracy, total_iou = 0, 0\n",
    "    for data in tqdm(loader):\n",
    "        data = data.to(device)\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            outs = model(data)\n",
    "\n",
    "            predicted_labels = outs.argmax(dim=1)\n",
    "            accuracy = predicted_labels.eq(data.y).sum().item() / data.num_nodes\n",
    "\n",
    "            sizes = (data.ptr[1:] - data.ptr[:-1]).tolist()\n",
    "            ious, categories = [], []\n",
    "            y_map = torch.empty(\n",
    "                loader.dataset.num_classes, device=device\n",
    "            ).long()\n",
    "            for out, y, category in zip(\n",
    "                outs.split(sizes), data.y.split(sizes), data.category.tolist()\n",
    "            ):\n",
    "                category = list(ShapeNet.seg_classes.keys())[category]\n",
    "                part = ShapeNet.seg_classes[category]\n",
    "                part = torch.tensor(part, device=device)\n",
    "                y_map[part] = torch.arange(part.size(0), device=device)\n",
    "                iou = jaccard_index(\n",
    "                    out[:, part].argmax(dim=-1), y_map[y],\n",
    "                    task=\"multiclass\", num_classes=part.size(0)\n",
    "                )\n",
    "                ious.append(iou)\n",
    "            categories.append(data.category)\n",
    "            iou = torch.tensor(ious, device=device)\n",
    "            category = torch.cat(categories, dim=0)\n",
    "            mean_iou = float(scatter(iou, category, reduce='mean').mean())\n",
    "\n",
    "            gt_pc_viz = data.pos.cpu().numpy().tolist()\n",
    "            segmentation_label = data.y.cpu().numpy().tolist()\n",
    "            frequency_dict = {key: 0 for key in segmentation_class_frequency.keys()}\n",
    "            for label in set(segmentation_label):\n",
    "                frequency_dict[label] = segmentation_label.count(label)\n",
    "            for j in range(len(gt_pc_viz)):\n",
    "                gt_pc_viz[j] += [segmentation_label[j] + 1 - class_offset]\n",
    "\n",
    "            predicted_pc_viz = data.pos.cpu().numpy().tolist()\n",
    "            segmentation_label = data.y.cpu().numpy().tolist()\n",
    "            frequency_dict = {key: 0 for key in segmentation_class_frequency.keys()}\n",
    "            for label in set(segmentation_label):\n",
    "                frequency_dict[label] = segmentation_label.count(label)\n",
    "            for j in range(len(predicted_pc_viz)):\n",
    "                predicted_pc_viz[j] += [segmentation_label[j] + 1 - class_offset]\n",
    "\n",
    "            table.add_data(\n",
    "                wandb.Object3D(np.array(gt_pc_viz)),\n",
    "                wandb.Object3D(np.array(predicted_pc_viz)),\n",
    "                accuracy, mean_iou, split, \"DGCNN\"\n",
    "            )\n",
    "            total_accuracy += accuracy\n",
    "            total_iou += mean_iou\n",
    "    \n",
    "    wandb.log({\n",
    "        f\"{split}/Accuracy\": total_accuracy / len(loader),\n",
    "        f\"{split}/IoU\": total_iou / len(loader),\n",
    "    })\n",
    "    \n",
    "    return table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c5f0bb-01b1-48a3-980d-c22434f2be8a",
   "metadata": {},
   "source": [
    "We evaluate the results and store them in a Weights & Biases Table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9427209c-216a-4887-9b3b-a73c474680c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f6f69c558644a988c42c473e64c39b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2349 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c87fdf230964f6d88cb4eb75b030908",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/341 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<wandb.data_types.Table at 0x7f5c80096c90>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table = wandb.Table(columns=[\"Ground-Truth\", \"Prediction\", \"Accuracy\", \"IoU\", \"Split\", \"Model-Name\"])\n",
    "evaluate(train_loader, \"Train-Val\", table)\n",
    "evaluate(test_loader, \"Test\", table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1688f54-ef52-4f42-a6f6-b5c3364da87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.log({\"Evaluation-Results\": table})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3dfd8610-bda8-4e7b-9939-b1e7b7b4e603",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Network error (ConnectTimeout), entering retry loop.\n",
      "wandb: Network error (ConnectTimeout), entering retry loop.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test/Accuracy</td><td>‚ñÅ</td></tr><tr><td>Test/IoU</td><td>‚ñÅ</td></tr><tr><td>Train-Val/Accuracy</td><td>‚ñÅ</td></tr><tr><td>Train-Val/IoU</td><td>‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test/Accuracy</td><td>0.90377</td></tr><tr><td>Test/IoU</td><td>0.73049</td></tr><tr><td>Train-Val/Accuracy</td><td>0.92111</td></tr><tr><td>Train-Val/IoU</td><td>0.80482</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">dgcnn</strong>: <a href=\"https://wandb.ai/wandb/point-cloud-segmentation/runs/m90yxuty\" target=\"_blank\">https://wandb.ai/wandb/point-cloud-segmentation/runs/m90yxuty</a><br/>Synced 6 W&B file(s), 1 media file(s), 2691 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20221220_110159-m90yxuty/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c44316-de14-4031-b6d2-ba8c9621a66d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "local-python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
