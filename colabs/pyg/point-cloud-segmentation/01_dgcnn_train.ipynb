{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/wandb/examples/blob/master/colabs/pyg/point-cloud-segmentation/01_dgcnn_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "<!--- @wandbcode{pyg-dgcnn-train} -->"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cdbbe526-23ee-4a90-af8c-9f7092f50192",
   "metadata": {},
   "source": [
    "# üî•üî• Train DGCNN Model using PyTorch Geometric and Weights & Biases ü™Ñüêù\n",
    "\n",
    "<!--- @wandbcode{pyg-dgcnn-train} -->\n",
    "\n",
    "[![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/wandb/examples/blob/pyg/point-cloud-segmentation/colabs/pyg/point-cloud-segmentation/01_dgcnn_train.ipynb)\n",
    "\n",
    "This notebook demonstrates an implementation of the [Dynamic Graph CNN](https://arxiv.org/pdf/1801.07829.pdf) for point cloud segmnetation implemented using [PyTorch Geometric](https://www.pyg.org/) and experiment tracked and visualized using [Weights & Biases](https://wandb.ai/site). The code here is inspired by [this](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/dgcnn_segmentation.py) original implementation.\n",
    "\n",
    "If you wish to know how to evaluate the model on the ShapeNetCore dataset using Weights & Biases, you can check out the following notebook:\n",
    "\n",
    "[![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/wandb/examples/blob/pyg/point-cloud-segmentation/colabs/pyg/point-cloud-segmentation/02_dgcnn_evaluate.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5c4e28-f40c-4c6a-827a-e28ff6db0501",
   "metadata": {},
   "source": [
    "# Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726d3107-dd0a-440a-a164-43b708af858f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "os.environ['TORCH'] = torch.__version__\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebab99e-383f-410d-93a0-c8ff29dad2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "!pip install -q torch-cluster -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n",
    "!pip install -q wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec996ce-98b9-465d-b530-e0f88ac7080d",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f2a4cd-24e6-457d-b7f6-7fc94912fd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wandb\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch_scatter import scatter\n",
    "from torchmetrics.functional import jaccard_index\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import ShapeNet\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import MLP, DynamicEdgeConv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71deb4e-f171-4a31-9801-a0e8f9cc9c04",
   "metadata": {},
   "source": [
    "# Initialize Weights & Biases\n",
    "\n",
    "We need to call [`wandb.init()`](https://docs.wandb.ai/ref/python/init) once at the beginning of our program to initialize a new job. This creates a new run in W&B and launches a background process to sync data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1053dec-59c9-4e9d-84e2-a1297dbf715f",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_project = \"pyg-point-cloud\" #@param {\"type\": \"string\"}\n",
    "wandb_run_name = \"train-dgcnn\" #@param {\"type\": \"string\"}\n",
    "\n",
    "wandb.init(project=wandb_project, name=wandb_run_name, job_type=\"train\")\n",
    "\n",
    "config = wandb.config\n",
    "\n",
    "config.seed = 42\n",
    "config.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "random.seed(config.seed)\n",
    "torch.manual_seed(config.seed)\n",
    "device = torch.device(config.device)\n",
    "\n",
    "config.category = 'Airplane' #@param [\"Bag\", \"Cap\", \"Car\", \"Chair\", \"Earphone\", \"Guitar\", \"Knife\", \"Lamp\", \"Laptop\", \"Motorbike\", \"Mug\", \"Pistol\", \"Rocket\", \"Skateboard\", \"Table\"] {type:\"raw\"}\n",
    "config.random_jitter_translation = 1e-2\n",
    "config.random_rotation_interval_x = 15\n",
    "config.random_rotation_interval_y = 15\n",
    "config.random_rotation_interval_z = 15\n",
    "config.validation_split = 0.2\n",
    "config.batch_size = 16\n",
    "config.num_workers = 6\n",
    "\n",
    "config.num_nearest_neighbours = 30\n",
    "config.aggregation_operator = \"max\"\n",
    "config.dropout = 0.5\n",
    "config.initial_lr = 1e-3\n",
    "config.lr_scheduler_step_size = 5\n",
    "config.gamma = 0.8\n",
    "\n",
    "config.epochs = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a72e20-ff1b-4cdf-b87f-10baff51bac7",
   "metadata": {},
   "source": [
    "# Load ShapeNet Dataset using PyTorch Geometric\n",
    "\n",
    "We now load, preprocess and batch the ModelNet dataset for training, validation/testing and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42955441-03d8-4e9f-b856-7d7edc708981",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = T.Compose([\n",
    "    T.RandomJitter(config.random_jitter_translation),\n",
    "    T.RandomRotate(config.random_rotation_interval_x, axis=0),\n",
    "    T.RandomRotate(config.random_rotation_interval_y, axis=1),\n",
    "    T.RandomRotate(config.random_rotation_interval_z, axis=2)\n",
    "])\n",
    "pre_transform = T.NormalizeScale()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da27bf0f-1e0f-438d-b602-2124606e7138",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = os.path.join('ShapeNet', config.category)\n",
    "\n",
    "train_val_dataset = ShapeNet(\n",
    "    dataset_path, config.category, split='trainval',\n",
    "    transform=transform, pre_transform=pre_transform\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e335648-933d-4865-aaaf-0f24c0358d16",
   "metadata": {},
   "source": [
    "Now, we need to offset the segmentation labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e14c4af-b5af-4805-a479-f0725d6ef1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentation_class_frequency = {}\n",
    "for idx in tqdm(range(len(train_val_dataset))):\n",
    "    pc_viz = train_val_dataset[idx].pos.numpy().tolist()\n",
    "    segmentation_label = train_val_dataset[idx].y.numpy().tolist()\n",
    "    for label in set(segmentation_label):\n",
    "        segmentation_class_frequency[label] = segmentation_label.count(label)\n",
    "class_offset = min(list(segmentation_class_frequency.keys()))\n",
    "print(\"Class Offset:\", class_offset)\n",
    "\n",
    "for idx in range(len(train_val_dataset)):\n",
    "    train_val_dataset[idx].y -= class_offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e09ff7-e64b-4912-a30b-461d5eb72cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_examples = int((1 - config.validation_split) * len(train_val_dataset))\n",
    "train_dataset = train_val_dataset[:num_train_examples]\n",
    "val_dataset = train_val_dataset[num_train_examples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bee5029-3bdd-4fa8-9a37-4d024c0cf6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=config.batch_size,\n",
    "    shuffle=True, num_workers=config.num_workers\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, batch_size=config.batch_size,\n",
    "    shuffle=False, num_workers=config.num_workers\n",
    ")\n",
    "visualization_loader = DataLoader(\n",
    "    val_dataset[:10], batch_size=1,\n",
    "    shuffle=False, num_workers=config.num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1bc429-7d54-411a-b477-b7bbc7bfc3d6",
   "metadata": {},
   "source": [
    "# Implementing the DGCNN Model using PyTorch Geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1bd7a8-585f-45fc-bc59-a8feec3512db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DGCNN(torch.nn.Module):\n",
    "    def __init__(self, out_channels, k=30, aggr='max'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = DynamicEdgeConv(MLP([2 * 6, 64, 64]), k, aggr)\n",
    "        self.conv2 = DynamicEdgeConv(MLP([2 * 64, 64, 64]), k, aggr)\n",
    "        self.conv3 = DynamicEdgeConv(MLP([2 * 64, 64, 64]), k, aggr)\n",
    "\n",
    "        self.mlp = MLP(\n",
    "            [3 * 64, 1024, 256, 128, out_channels],\n",
    "            dropout=0.5, norm=None\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, pos, batch = data.x, data.pos, data.batch\n",
    "        x0 = torch.cat([x, pos], dim=-1)\n",
    "        \n",
    "        x1 = self.conv1(x0, batch)\n",
    "        x2 = self.conv2(x1, batch)\n",
    "        x3 = self.conv3(x2, batch)\n",
    "        \n",
    "        out = self.mlp(torch.cat([x1, x2, x3], dim=1))\n",
    "        return F.log_softmax(out, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a677e7b-9179-4c71-b1dc-af636dcf95d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.num_classes = train_dataset.num_classes\n",
    "\n",
    "model = DGCNN(\n",
    "    out_channels=train_dataset.num_classes,\n",
    "    k=config.num_nearest_neighbours,\n",
    "    aggr=config.aggregation_operator\n",
    ").to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config.initial_lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer, step_size=config.lr_scheduler_step_size, gamma=config.gamma\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d94603b-51df-4299-ae69-f43617827d1c",
   "metadata": {},
   "source": [
    "# Training DGCNN and Logging Metrics on Weights & Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9fbe02-b095-4df7-ab6b-af0dd6a411bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(epoch):\n",
    "    model.train()\n",
    "    \n",
    "    ious, categories = [], []\n",
    "    total_loss = correct_nodes = total_nodes = 0\n",
    "    y_map = torch.empty(\n",
    "        train_loader.dataset.num_classes, device=device\n",
    "    ).long()\n",
    "    num_train_examples = len(train_loader)\n",
    "    \n",
    "    progress_bar = tqdm(\n",
    "        train_loader, desc=f\"Training Epoch {epoch}/{config.epochs}\"\n",
    "    )\n",
    "    \n",
    "    for data in progress_bar:\n",
    "        data = data.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outs = model(data)\n",
    "        loss = F.nll_loss(outs, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        correct_nodes += outs.argmax(dim=1).eq(data.y).sum().item()\n",
    "        total_nodes += data.num_nodes\n",
    "        \n",
    "        sizes = (data.ptr[1:] - data.ptr[:-1]).tolist()\n",
    "        for out, y, category in zip(outs.split(sizes), data.y.split(sizes),\n",
    "                                    data.category.tolist()):\n",
    "            category = list(ShapeNet.seg_classes.keys())[category]\n",
    "            part = ShapeNet.seg_classes[category]\n",
    "            part = torch.tensor(part, device=device)\n",
    "\n",
    "            y_map[part] = torch.arange(part.size(0), device=device)\n",
    "\n",
    "            iou = jaccard_index(\n",
    "                out[:, part].argmax(dim=-1), y_map[y],\n",
    "                task=\"multiclass\", num_classes=part.size(0)\n",
    "            )\n",
    "            ious.append(iou)\n",
    "\n",
    "        categories.append(data.category)\n",
    "        \n",
    "    iou = torch.tensor(ious, device=device)\n",
    "    category = torch.cat(categories, dim=0)\n",
    "    mean_iou = float(scatter(iou, category, reduce='mean').mean())\n",
    "    \n",
    "    return {\n",
    "        \"Train/Loss\": total_loss / num_train_examples,\n",
    "        \"Train/Accuracy\": correct_nodes / total_nodes,\n",
    "        \"Train/IoU\": mean_iou\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d7e550-19b1-40b8-9e01-2997a694a4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def val_step(epoch):\n",
    "    model.eval()\n",
    "\n",
    "    ious, categories = [], []\n",
    "    total_loss = correct_nodes = total_nodes = 0\n",
    "    y_map = torch.empty(\n",
    "        val_loader.dataset.num_classes, device=device\n",
    "    ).long()\n",
    "    num_val_examples = len(val_loader)\n",
    "    \n",
    "    progress_bar = tqdm(\n",
    "        val_loader, desc=f\"Validating Epoch {epoch}/{config.epochs}\"\n",
    "    )\n",
    "    \n",
    "    for data in progress_bar:\n",
    "        data = data.to(device)\n",
    "        outs = model(data)\n",
    "        \n",
    "        loss = F.nll_loss(outs, data.y)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        correct_nodes += outs.argmax(dim=1).eq(data.y).sum().item()\n",
    "        total_nodes += data.num_nodes\n",
    "\n",
    "        sizes = (data.ptr[1:] - data.ptr[:-1]).tolist()\n",
    "        for out, y, category in zip(outs.split(sizes), data.y.split(sizes),\n",
    "                                    data.category.tolist()):\n",
    "            category = list(ShapeNet.seg_classes.keys())[category]\n",
    "            part = ShapeNet.seg_classes[category]\n",
    "            part = torch.tensor(part, device=device)\n",
    "\n",
    "            y_map[part] = torch.arange(part.size(0), device=device)\n",
    "\n",
    "            iou = jaccard_index(\n",
    "                out[:, part].argmax(dim=-1), y_map[y],\n",
    "                task=\"multiclass\", num_classes=part.size(0)\n",
    "            )\n",
    "            ious.append(iou)\n",
    "\n",
    "        categories.append(data.category)\n",
    "\n",
    "    iou = torch.tensor(ious, device=device)\n",
    "    category = torch.cat(categories, dim=0)\n",
    "    mean_iou = float(scatter(iou, category, reduce='mean').mean())\n",
    "    \n",
    "    return {\n",
    "        \"Validation/Loss\": total_loss / num_val_examples,\n",
    "        \"Validation/Accuracy\": correct_nodes / total_nodes,\n",
    "        \"Validation/IoU\": mean_iou\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4fad76-f961-4a0a-9f37-c72c1422e885",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def visualization_step(epoch, table):\n",
    "    model.eval()\n",
    "    for data in tqdm(visualization_loader):\n",
    "        data = data.to(device)\n",
    "        outs = model(data)\n",
    "\n",
    "        predicted_labels = outs.argmax(dim=1)\n",
    "        accuracy = predicted_labels.eq(data.y).sum().item() / data.num_nodes\n",
    "\n",
    "        sizes = (data.ptr[1:] - data.ptr[:-1]).tolist()\n",
    "        ious, categories = [], []\n",
    "        y_map = torch.empty(\n",
    "            visualization_loader.dataset.num_classes, device=device\n",
    "        ).long()\n",
    "        for out, y, category in zip(\n",
    "            outs.split(sizes), data.y.split(sizes), data.category.tolist()\n",
    "        ):\n",
    "            category = list(ShapeNet.seg_classes.keys())[category]\n",
    "            part = ShapeNet.seg_classes[category]\n",
    "            part = torch.tensor(part, device=device)\n",
    "            y_map[part] = torch.arange(part.size(0), device=device)\n",
    "            iou = jaccard_index(\n",
    "                out[:, part].argmax(dim=-1), y_map[y],\n",
    "                task=\"multiclass\", num_classes=part.size(0)\n",
    "            )\n",
    "            ious.append(iou)\n",
    "        categories.append(data.category)\n",
    "        iou = torch.tensor(ious, device=device)\n",
    "        category = torch.cat(categories, dim=0)\n",
    "        mean_iou = float(scatter(iou, category, reduce='mean').mean())\n",
    "\n",
    "        gt_pc_viz = data.pos.cpu().numpy().tolist()\n",
    "        segmentation_label = data.y.cpu().numpy().tolist()\n",
    "        frequency_dict = {key: 0 for key in segmentation_class_frequency.keys()}\n",
    "        for label in set(segmentation_label):\n",
    "            frequency_dict[label] = segmentation_label.count(label)\n",
    "        for j in range(len(gt_pc_viz)):\n",
    "            # gt_pc_viz[j] += [segmentation_label[j] + 1 - class_offset]\n",
    "            gt_pc_viz[j] += [segmentation_label[j] + 1]\n",
    "\n",
    "        predicted_pc_viz = data.pos.cpu().numpy().tolist()\n",
    "        segmentation_label = data.y.cpu().numpy().tolist()\n",
    "        frequency_dict = {key: 0 for key in segmentation_class_frequency.keys()}\n",
    "        for label in set(segmentation_label):\n",
    "            frequency_dict[label] = segmentation_label.count(label)\n",
    "        for j in range(len(predicted_pc_viz)):\n",
    "            # predicted_pc_viz[j] += [segmentation_label[j] + 1 - class_offset]\n",
    "            predicted_pc_viz[j] += [segmentation_label[j] + 1]\n",
    "\n",
    "        table.add_data(\n",
    "            epoch, wandb.Object3D(np.array(gt_pc_viz)),\n",
    "            wandb.Object3D(np.array(predicted_pc_viz)),\n",
    "            accuracy, mean_iou\n",
    "        )\n",
    "    \n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c31a63-96a4-409f-bb0d-2b56b034adf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(epoch):\n",
    "    \"\"\"Save model checkpoints as Weights & Biases artifacts\"\"\"\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()\n",
    "    }, \"checkpoint.pt\")\n",
    "    \n",
    "    artifact_name = wandb.util.make_artifact_name_safe(\n",
    "        f\"{wandb.run.name}-{wandb.run.id}-checkpoint\"\n",
    "    )\n",
    "    \n",
    "    checkpoint_artifact = wandb.Artifact(artifact_name, type=\"checkpoint\")\n",
    "    checkpoint_artifact.add_file(\"checkpoint.pt\")\n",
    "    wandb.log_artifact(\n",
    "        checkpoint_artifact, aliases=[\"latest\", f\"epoch-{epoch}\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f013430-fd25-4541-9dd3-3c67eef9074a",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = wandb.Table(columns=[\"Epoch\", \"Ground-Truth\", \"Prediction\", \"Accuracy\", \"IoU\"])\n",
    "\n",
    "for epoch in range(1, config.epochs + 1):\n",
    "    train_metrics = train_step(epoch)\n",
    "    val_metrics = val_step(epoch)\n",
    "    \n",
    "    metrics = {**train_metrics, **val_metrics}\n",
    "    metrics[\"learning_rate\"] = scheduler.get_last_lr()[-1]\n",
    "    wandb.log(metrics)\n",
    "    \n",
    "    table = visualization_step(epoch, table)\n",
    "    \n",
    "    scheduler.step()\n",
    "    save_checkpoint(epoch)\n",
    "\n",
    "wandb.log({\"Evaluation\": table})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000c8944-6821-43a3-94d2-548f8b174334",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b093dee2-e902-4ad8-af3c-d7126ef985c1",
   "metadata": {},
   "source": [
    "Next, you can check out the following notebook to learn how to evaluate the model on the ShapeNetCore dataset using Weights & Biases, you can check out the following notebook:\n",
    "\n",
    "[![](https://colab.research.google.com/assets/colab-badge.svg)]()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
