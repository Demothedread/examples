{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cdbbe526-23ee-4a90-af8c-9f7092f50192",
   "metadata": {},
   "source": [
    "# üî•üî• Train DGCNN Model using PyTorch Geometric and Weights & Biases ü™Ñüêù\n",
    "\n",
    "<!--- @wandbcode{pyg-dgcnn-train} -->\n",
    "\n",
    "[![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/wandb/examples/blob/pyg/point-cloud-segmentation/colabs/pyg/point-cloud-segmentation/01_dgcnn_train.ipynb)\n",
    "\n",
    "This notebook demonstrates an implementation of the [Dynamic Graph CNN](https://arxiv.org/pdf/1801.07829.pdf) for point cloud segmnetation implemented using [PyTorch Geometric](https://www.pyg.org/) and experiment tracked and visualized using [Weights & Biases](https://wandb.ai/site). The code here is inspired by [this](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/dgcnn_segmentation.py) original implementation.\n",
    "\n",
    "If you wish to know how to evaluate the model on the ShapeNetCore dataset using Weights & Biases, you can check out the following notebook:\n",
    "\n",
    "[![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/wandb/examples/blob/pyg/point-cloud-segmentation/colabs/pyg/point-cloud-segmentation/02_dgcnn_evaluate.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5c4e28-f40c-4c6a-827a-e28ff6db0501",
   "metadata": {},
   "source": [
    "# Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726d3107-dd0a-440a-a164-43b708af858f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "os.environ['TORCH'] = torch.__version__\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebab99e-383f-410d-93a0-c8ff29dad2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "!pip install -q torch-cluster -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n",
    "!pip install -q wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec996ce-98b9-465d-b530-e0f88ac7080d",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7f2a4cd-24e6-457d-b7f6-7fc94912fd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wandb\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch_scatter import scatter\n",
    "from torchmetrics.functional import jaccard_index\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import ShapeNet\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import MLP, DynamicEdgeConv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71deb4e-f171-4a31-9801-a0e8f9cc9c04",
   "metadata": {},
   "source": [
    "# Initialize Weights & Biases\n",
    "\n",
    "We need to call [`wandb.init()`](https://docs.wandb.ai/ref/python/init) once at the beginning of our program to initialize a new job. This creates a new run in W&B and launches a background process to sync data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1053dec-59c9-4e9d-84e2-a1297dbf715f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgeekyrakshit\u001b[0m (\u001b[33mwandb\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jupyter/segmentation/wandb/run-20221221_123936-2tjrb7xy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/wandb/point-cloud-segmentation/runs/2tjrb7xy\" target=\"_blank\">avid-aardvark-57</a></strong> to <a href=\"https://wandb.ai/wandb/point-cloud-segmentation\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb_project = \"pyg-point-cloud\" #@param {\"type\": \"string\"}\n",
    "wandb_run_name = \"train-dgcnn\" #@param {\"type\": \"string\"}\n",
    "\n",
    "wandb.init(project=wandb_project, name=wandb_run_name, job_type=\"train\")\n",
    "\n",
    "config = wandb.config\n",
    "\n",
    "config.seed = 42\n",
    "config.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "random.seed(config.seed)\n",
    "torch.manual_seed(config.seed)\n",
    "device = torch.device(config.device)\n",
    "\n",
    "config.category = 'Airplane' #@param [\"Bag\", \"Cap\", \"Car\", \"Chair\", \"Earphone\", \"Guitar\", \"Knife\", \"Lamp\", \"Laptop\", \"Motorbike\", \"Mug\", \"Pistol\", \"Rocket\", \"Skateboard\", \"Table\"] {type:\"raw\"}\n",
    "config.random_jitter_translation = 1e-2\n",
    "config.random_rotation_interval_x = 15\n",
    "config.random_rotation_interval_y = 15\n",
    "config.random_rotation_interval_z = 15\n",
    "config.validation_split = 0.2\n",
    "config.batch_size = 16\n",
    "config.num_workers = 6\n",
    "\n",
    "config.num_nearest_neighbours = 30\n",
    "config.aggregation_operator = \"max\"\n",
    "config.dropout = 0.5\n",
    "config.initial_lr = 1e-3\n",
    "config.lr_scheduler_step_size = 5\n",
    "config.gamma = 0.8\n",
    "\n",
    "config.epochs = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a72e20-ff1b-4cdf-b87f-10baff51bac7",
   "metadata": {},
   "source": [
    "# Load ShapeNet Dataset using PyTorch Geometric\n",
    "\n",
    "We now load, preprocess and batch the ModelNet dataset for training, validation/testing and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42955441-03d8-4e9f-b856-7d7edc708981",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = T.Compose([\n",
    "    T.RandomJitter(config.random_jitter_translation),\n",
    "    T.RandomRotate(config.random_rotation_interval_x, axis=0),\n",
    "    T.RandomRotate(config.random_rotation_interval_y, axis=1),\n",
    "    T.RandomRotate(config.random_rotation_interval_z, axis=2)\n",
    "])\n",
    "pre_transform = T.NormalizeScale()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da27bf0f-1e0f-438d-b602-2124606e7138",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch_geometric/data/dataset.py:191: UserWarning: The `pre_transform` argument differs from the one used in the pre-processed version of this dataset. If you want to make use of another pre-processing technique, make sure to delete 'ShapeNet/Car/processed' first\n",
      "  f\"The `pre_transform` argument differs from the one used in \"\n"
     ]
    }
   ],
   "source": [
    "dataset_path = os.path.join('ShapeNet', config.category)\n",
    "\n",
    "train_val_dataset = ShapeNet(\n",
    "    dataset_path, config.category, split='trainval',\n",
    "    transform=transform, pre_transform=pre_transform\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e335648-933d-4865-aaaf-0f24c0358d16",
   "metadata": {},
   "source": [
    "Now, we need to offset the segmentation labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e14c4af-b5af-4805-a479-f0725d6ef1de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "817c25c0438749e28ef26a16e7c12d3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/740 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Offset: 8\n"
     ]
    }
   ],
   "source": [
    "segmentation_class_frequency = {}\n",
    "for idx in tqdm(range(len(train_val_dataset))):\n",
    "    pc_viz = train_val_dataset[idx].pos.numpy().tolist()\n",
    "    segmentation_label = train_val_dataset[idx].y.numpy().tolist()\n",
    "    for label in set(segmentation_label):\n",
    "        segmentation_class_frequency[label] = segmentation_label.count(label)\n",
    "class_offset = min(list(segmentation_class_frequency.keys()))\n",
    "print(\"Class Offset:\", class_offset)\n",
    "\n",
    "for idx in range(len(train_val_dataset)):\n",
    "    train_val_dataset[idx].y -= class_offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10e09ff7-e64b-4912-a30b-461d5eb72cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_examples = int((1 - config.validation_split) * len(train_val_dataset))\n",
    "train_dataset = train_val_dataset[:num_train_examples]\n",
    "val_dataset = train_val_dataset[num_train_examples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8bee5029-3bdd-4fa8-9a37-4d024c0cf6b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=config.batch_size,\n",
    "    shuffle=True, num_workers=config.num_workers\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, batch_size=config.batch_size,\n",
    "    shuffle=False, num_workers=config.num_workers\n",
    ")\n",
    "visualization_loader = DataLoader(\n",
    "    val_dataset[:10], batch_size=1,\n",
    "    shuffle=False, num_workers=config.num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1bc429-7d54-411a-b477-b7bbc7bfc3d6",
   "metadata": {},
   "source": [
    "# Implementing the DGCNN Model using PyTorch Geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac1bd7a8-585f-45fc-bc59-a8feec3512db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DGCNN(torch.nn.Module):\n",
    "    def __init__(self, out_channels, k=30, aggr='max'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = DynamicEdgeConv(MLP([2 * 6, 64, 64]), k, aggr)\n",
    "        self.conv2 = DynamicEdgeConv(MLP([2 * 64, 64, 64]), k, aggr)\n",
    "        self.conv3 = DynamicEdgeConv(MLP([2 * 64, 64, 64]), k, aggr)\n",
    "\n",
    "        self.mlp = MLP(\n",
    "            [3 * 64, 1024, 256, 128, out_channels],\n",
    "            dropout=0.5, norm=None\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, pos, batch = data.x, data.pos, data.batch\n",
    "        x0 = torch.cat([x, pos], dim=-1)\n",
    "        \n",
    "        x1 = self.conv1(x0, batch)\n",
    "        x2 = self.conv2(x1, batch)\n",
    "        x3 = self.conv3(x2, batch)\n",
    "        \n",
    "        out = self.mlp(torch.cat([x1, x2, x3], dim=1))\n",
    "        return F.log_softmax(out, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a677e7b-9179-4c71-b1dc-af636dcf95d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.num_classes = train_dataset.num_classes\n",
    "\n",
    "model = DGCNN(\n",
    "    out_channels=train_dataset.num_classes,\n",
    "    k=config.num_nearest_neighbours,\n",
    "    aggr=config.aggregation_operator\n",
    ").to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config.initial_lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer, step_size=config.lr_scheduler_step_size, gamma=config.gamma\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d94603b-51df-4299-ae69-f43617827d1c",
   "metadata": {},
   "source": [
    "# Training DGCNN and Logging Metrics on Weights & Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a9fbe02-b095-4df7-ab6b-af0dd6a411bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(epoch):\n",
    "    model.train()\n",
    "    \n",
    "    ious, categories = [], []\n",
    "    total_loss = correct_nodes = total_nodes = 0\n",
    "    y_map = torch.empty(\n",
    "        train_loader.dataset.num_classes, device=device\n",
    "    ).long()\n",
    "    num_train_examples = len(train_loader)\n",
    "    \n",
    "    progress_bar = tqdm(\n",
    "        train_loader, desc=f\"Training Epoch {epoch}/{config.epochs}\"\n",
    "    )\n",
    "    \n",
    "    for data in progress_bar:\n",
    "        data = data.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outs = model(data)\n",
    "        loss = F.nll_loss(outs, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        correct_nodes += outs.argmax(dim=1).eq(data.y).sum().item()\n",
    "        total_nodes += data.num_nodes\n",
    "        \n",
    "        sizes = (data.ptr[1:] - data.ptr[:-1]).tolist()\n",
    "        for out, y, category in zip(outs.split(sizes), data.y.split(sizes),\n",
    "                                    data.category.tolist()):\n",
    "            category = list(ShapeNet.seg_classes.keys())[category]\n",
    "            part = ShapeNet.seg_classes[category]\n",
    "            part = torch.tensor(part, device=device)\n",
    "\n",
    "            y_map[part] = torch.arange(part.size(0), device=device)\n",
    "\n",
    "            iou = jaccard_index(\n",
    "                out[:, part].argmax(dim=-1), y_map[y],\n",
    "                task=\"multiclass\", num_classes=part.size(0)\n",
    "            )\n",
    "            ious.append(iou)\n",
    "\n",
    "        categories.append(data.category)\n",
    "        \n",
    "    iou = torch.tensor(ious, device=device)\n",
    "    category = torch.cat(categories, dim=0)\n",
    "    mean_iou = float(scatter(iou, category, reduce='mean').mean())\n",
    "    \n",
    "    return {\n",
    "        \"Train/Loss\": total_loss / num_train_examples,\n",
    "        \"Train/Accuracy\": correct_nodes / total_nodes,\n",
    "        \"Train/IoU\": mean_iou\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94d7e550-19b1-40b8-9e01-2997a694a4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def val_step(epoch):\n",
    "    model.eval()\n",
    "\n",
    "    ious, categories = [], []\n",
    "    total_loss = correct_nodes = total_nodes = 0\n",
    "    y_map = torch.empty(\n",
    "        val_loader.dataset.num_classes, device=device\n",
    "    ).long()\n",
    "    num_val_examples = len(val_loader)\n",
    "    \n",
    "    progress_bar = tqdm(\n",
    "        val_loader, desc=f\"Validating Epoch {epoch}/{config.epochs}\"\n",
    "    )\n",
    "    \n",
    "    for data in progress_bar:\n",
    "        data = data.to(device)\n",
    "        outs = model(data)\n",
    "        \n",
    "        loss = F.nll_loss(outs, data.y)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        correct_nodes += outs.argmax(dim=1).eq(data.y).sum().item()\n",
    "        total_nodes += data.num_nodes\n",
    "\n",
    "        sizes = (data.ptr[1:] - data.ptr[:-1]).tolist()\n",
    "        for out, y, category in zip(outs.split(sizes), data.y.split(sizes),\n",
    "                                    data.category.tolist()):\n",
    "            category = list(ShapeNet.seg_classes.keys())[category]\n",
    "            part = ShapeNet.seg_classes[category]\n",
    "            part = torch.tensor(part, device=device)\n",
    "\n",
    "            y_map[part] = torch.arange(part.size(0), device=device)\n",
    "\n",
    "            iou = jaccard_index(\n",
    "                out[:, part].argmax(dim=-1), y_map[y],\n",
    "                task=\"multiclass\", num_classes=part.size(0)\n",
    "            )\n",
    "            ious.append(iou)\n",
    "\n",
    "        categories.append(data.category)\n",
    "\n",
    "    iou = torch.tensor(ious, device=device)\n",
    "    category = torch.cat(categories, dim=0)\n",
    "    mean_iou = float(scatter(iou, category, reduce='mean').mean())\n",
    "    \n",
    "    return {\n",
    "        \"Validation/Loss\": total_loss / num_val_examples,\n",
    "        \"Validation/Accuracy\": correct_nodes / total_nodes,\n",
    "        \"Validation/IoU\": mean_iou\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da4fad76-f961-4a0a-9f37-c72c1422e885",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def visualization_step(epoch, table):\n",
    "    model.eval()\n",
    "    for data in tqdm(visualization_loader):\n",
    "        data = data.to(device)\n",
    "        outs = model(data)\n",
    "\n",
    "        predicted_labels = outs.argmax(dim=1)\n",
    "        accuracy = predicted_labels.eq(data.y).sum().item() / data.num_nodes\n",
    "\n",
    "        sizes = (data.ptr[1:] - data.ptr[:-1]).tolist()\n",
    "        ious, categories = [], []\n",
    "        y_map = torch.empty(\n",
    "            visualization_loader.dataset.num_classes, device=device\n",
    "        ).long()\n",
    "        for out, y, category in zip(\n",
    "            outs.split(sizes), data.y.split(sizes), data.category.tolist()\n",
    "        ):\n",
    "            category = list(ShapeNet.seg_classes.keys())[category]\n",
    "            part = ShapeNet.seg_classes[category]\n",
    "            part = torch.tensor(part, device=device)\n",
    "            y_map[part] = torch.arange(part.size(0), device=device)\n",
    "            iou = jaccard_index(\n",
    "                out[:, part].argmax(dim=-1), y_map[y],\n",
    "                task=\"multiclass\", num_classes=part.size(0)\n",
    "            )\n",
    "            ious.append(iou)\n",
    "        categories.append(data.category)\n",
    "        iou = torch.tensor(ious, device=device)\n",
    "        category = torch.cat(categories, dim=0)\n",
    "        mean_iou = float(scatter(iou, category, reduce='mean').mean())\n",
    "\n",
    "        gt_pc_viz = data.pos.cpu().numpy().tolist()\n",
    "        segmentation_label = data.y.cpu().numpy().tolist()\n",
    "        frequency_dict = {key: 0 for key in segmentation_class_frequency.keys()}\n",
    "        for label in set(segmentation_label):\n",
    "            frequency_dict[label] = segmentation_label.count(label)\n",
    "        for j in range(len(gt_pc_viz)):\n",
    "            # gt_pc_viz[j] += [segmentation_label[j] + 1 - class_offset]\n",
    "            gt_pc_viz[j] += [segmentation_label[j] + 1]\n",
    "\n",
    "        predicted_pc_viz = data.pos.cpu().numpy().tolist()\n",
    "        segmentation_label = data.y.cpu().numpy().tolist()\n",
    "        frequency_dict = {key: 0 for key in segmentation_class_frequency.keys()}\n",
    "        for label in set(segmentation_label):\n",
    "            frequency_dict[label] = segmentation_label.count(label)\n",
    "        for j in range(len(predicted_pc_viz)):\n",
    "            # predicted_pc_viz[j] += [segmentation_label[j] + 1 - class_offset]\n",
    "            predicted_pc_viz[j] += [segmentation_label[j] + 1]\n",
    "\n",
    "        table.add_data(\n",
    "            epoch, wandb.Object3D(np.array(gt_pc_viz)),\n",
    "            wandb.Object3D(np.array(predicted_pc_viz)),\n",
    "            accuracy, mean_iou\n",
    "        )\n",
    "    \n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8c31a63-96a4-409f-bb0d-2b56b034adf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(epoch):\n",
    "    \"\"\"Save model checkpoints as Weights & Biases artifacts\"\"\"\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()\n",
    "    }, \"checkpoint.pt\")\n",
    "    \n",
    "    artifact_name = wandb.util.make_artifact_name_safe(\n",
    "        f\"{wandb.run.name}-{wandb.run.id}-checkpoint\"\n",
    "    )\n",
    "    \n",
    "    checkpoint_artifact = wandb.Artifact(artifact_name, type=\"checkpoint\")\n",
    "    checkpoint_artifact.add_file(\"checkpoint.pt\")\n",
    "    wandb.log_artifact(\n",
    "        checkpoint_artifact, aliases=[\"latest\", f\"epoch-{epoch}\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f013430-fd25-4541-9dd3-3c67eef9074a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b285a96843a54355a7b9952810e6d3fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 1/1:   0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a949a8afe964cf1a8d6de94e8cd02e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating Epoch 1/1:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6ff58a69d854e788cd8dac9dc00ad1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "table = wandb.Table(columns=[\"Epoch\", \"Ground-Truth\", \"Prediction\", \"Accuracy\", \"IoU\"])\n",
    "\n",
    "for epoch in range(1, config.epochs + 1):\n",
    "    train_metrics = train_step(epoch)\n",
    "    val_metrics = val_step(epoch)\n",
    "    \n",
    "    metrics = {**train_metrics, **val_metrics}\n",
    "    metrics[\"learning_rate\"] = scheduler.get_last_lr()[-1]\n",
    "    wandb.log(metrics)\n",
    "    \n",
    "    table = visualization_step(epoch, table)\n",
    "    \n",
    "    scheduler.step()\n",
    "    save_checkpoint(epoch)\n",
    "\n",
    "wandb.log({\"Evaluation\": table})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "000c8944-6821-43a3-94d2-548f8b174334",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train/Accuracy</td><td>‚ñÅ</td></tr><tr><td>Train/IoU</td><td>‚ñÅ</td></tr><tr><td>Train/Loss</td><td>‚ñÅ</td></tr><tr><td>Validation/Accuracy</td><td>‚ñÅ</td></tr><tr><td>Validation/IoU</td><td>‚ñÅ</td></tr><tr><td>Validation/Loss</td><td>‚ñÅ</td></tr><tr><td>learning_rate</td><td>‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train/Accuracy</td><td>0.65238</td></tr><tr><td>Train/IoU</td><td>0.18712</td></tr><tr><td>Train/Loss</td><td>1.3871</td></tr><tr><td>Validation/Accuracy</td><td>0.72128</td></tr><tr><td>Validation/IoU</td><td>0.17988</td></tr><tr><td>Validation/Loss</td><td>0.72997</td></tr><tr><td>learning_rate</td><td>0.001</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">avid-aardvark-57</strong>: <a href=\"https://wandb.ai/wandb/point-cloud-segmentation/runs/2tjrb7xy\" target=\"_blank\">https://wandb.ai/wandb/point-cloud-segmentation/runs/2tjrb7xy</a><br/>Synced 6 W&B file(s), 1 media file(s), 12 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20221221_123936-2tjrb7xy/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b093dee2-e902-4ad8-af3c-d7126ef985c1",
   "metadata": {},
   "source": [
    "Next, you can check out the following notebook to learn how to evaluate the model on the ShapeNetCore dataset using Weights & Biases, you can check out the following notebook:\n",
    "\n",
    "[![](https://colab.research.google.com/assets/colab-badge.svg)]()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Apr  5 2022, 01:52:34) \n[Clang 12.0.0 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "c46d4533a0cf80e4c6573b8f3c1bd70be2b87d9445b1cb498494120f873c7315"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
