{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpLZ1LnA2dQ5"
      },
      "source": [
        "## Imports and Setups"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rf2eHZT16Iob"
      },
      "source": [
        "!pip install -qq wandb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRTx1-Zp7LEn"
      },
      "source": [
        "import wandb\n",
        "from wandb.keras import WandbMetricsLogger\n",
        "from wandb.keras import WandbEvalCallback\n",
        "\n",
        "wandb.login()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3RZuWEXx4LrD"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import models\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "from argparse import Namespace\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "configs = Namespace(\n",
        "    img_size = 128,\n",
        "    batch_size = 32,\n",
        "    num_classes = 3,\n",
        ")\n",
        "configs"
      ],
      "metadata": {
        "id": "iTmWoWxXAlGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataloader\n",
        "\n",
        "We will be using Oxford Pets Dataset which we can directly get from TensorFlow Datasets."
      ],
      "metadata": {
        "id": "x7LMKima_Lh_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds, valid_ds = tfds.load('oxford_iiit_pet', split=[\"train\", \"test\"])"
      ],
      "metadata": {
        "id": "Ewb18OyD-LVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pgrpfqWIeI-"
      },
      "source": [
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "\n",
        "def parse_data(example):\n",
        "    # Parse image\n",
        "    image = example[\"image\"]\n",
        "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
        "    image = tf.image.resize(image, size=(configs.img_size, configs.img_size))\n",
        "\n",
        "    # Parse mask\n",
        "    mask = example[\"segmentation_mask\"] - 1 # ground truth labels are [1,2,3].\n",
        "    mask = tf.image.resize(mask, size=(configs.img_size, configs.img_size), method='nearest')\n",
        "    mask = tf.one_hot(tf.squeeze(mask, axis=-1), depth=configs.num_classes)\n",
        "\n",
        "    return image, mask\n",
        "\n",
        "trainloader = (\n",
        "    train_ds\n",
        "    .shuffle(1024)\n",
        "    .map(parse_data, num_parallel_calls=AUTOTUNE)\n",
        "    .batch(configs.batch_size)\n",
        "    .prefetch(AUTOTUNE)\n",
        ")\n",
        "\n",
        "validloader = (\n",
        "    valid_ds\n",
        "    .map(parse_data, num_parallel_calls=AUTOTUNE)\n",
        "    .batch(configs.batch_size)\n",
        "    .prefetch(AUTOTUNE)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5kZhpJks2J5"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPkmeHXRY_MU"
      },
      "source": [
        "# ref: https://github.com/ayulockin/deepimageinpainting/blob/master/Image_Inpainting_Autoencoder_Decoder_v2_0.ipynb\n",
        "class SegmentationModel:\n",
        "    '''\n",
        "    Build UNET based model for segmentation task.\n",
        "    '''\n",
        "    def prepare_model(self, OUTPUT_CHANNEL, input_size=(configs.img_size, configs.img_size, 3)):\n",
        "        inputs = layers.Input(input_size)\n",
        "\n",
        "        conv1, pool1 = self.__ConvBlock(32, (3,3), (2,2), 'relu', 'same', inputs)\n",
        "        conv2, pool2 = self.__ConvBlock(64, (3,3), (2,2), 'relu', 'same', pool1)\n",
        "        conv3, pool3 = self.__ConvBlock(128, (3,3), (2,2), 'relu', 'same', pool2)\n",
        "        conv4, pool4 = self.__ConvBlock(256, (3,3), (2,2), 'relu', 'same', pool3)\n",
        "        \n",
        "        conv5, up6 = self.__UpConvBlock(512, 256, (3,3), (2,2), (2,2), 'relu', 'same', pool4, conv4)\n",
        "        conv6, up7 = self.__UpConvBlock(256, 128, (3,3), (2,2), (2,2), 'relu', 'same', up6, conv3)\n",
        "        conv7, up8 = self.__UpConvBlock(128, 64, (3,3), (2,2), (2,2), 'relu', 'same', up7, conv2)\n",
        "        conv8, up9 = self.__UpConvBlock(64, 32, (3,3), (2,2), (2,2), 'relu', 'same', up8, conv1)\n",
        "\n",
        "        conv9 = self.__ConvBlock(32, (3,3), (2,2), 'relu', 'same', up9, False)\n",
        "        \n",
        "        outputs = layers.Conv2D(OUTPUT_CHANNEL, (3, 3), activation='softmax', padding='same')(conv9)\n",
        "\n",
        "        return models.Model(inputs=[inputs], outputs=[outputs])  \n",
        "\n",
        "    def __ConvBlock(self, filters, kernel_size, pool_size, activation, padding, connecting_layer, pool_layer=True):\n",
        "        conv = layers.Conv2D(filters=filters, kernel_size=kernel_size, activation=activation, padding=padding)(connecting_layer)\n",
        "        conv = layers.Conv2D(filters=filters, kernel_size=kernel_size, activation=activation, padding=padding)(conv)\n",
        "        if pool_layer:\n",
        "          pool = layers.MaxPooling2D(pool_size)(conv)\n",
        "          return conv, pool\n",
        "        else:\n",
        "          return conv\n",
        "\n",
        "    def __UpConvBlock(self, filters, up_filters, kernel_size, up_kernel, up_stride, activation, padding, connecting_layer, shared_layer):\n",
        "        conv = layers.Conv2D(filters=filters, kernel_size=kernel_size, activation=activation, padding=padding)(connecting_layer)\n",
        "        conv = layers.Conv2D(filters=filters, kernel_size=kernel_size, activation=activation, padding=padding)(conv)\n",
        "        up = layers.Conv2DTranspose(filters=up_filters, kernel_size=up_kernel, strides=up_stride, padding=padding)(conv)\n",
        "        up = layers.concatenate([up, shared_layer], axis=3)\n",
        "\n",
        "        return conv, up"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VDkcXkjs5TV"
      },
      "source": [
        "#### Initialize Model and Compile"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPpOC1NXZwsI"
      },
      "source": [
        "# output channel is 3 because we have three classes in our mask\n",
        "tf.keras.backend.clear_session()\n",
        "model = SegmentationModel().prepare_model(configs.num_classes)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=\"categorical_crossentropy\",\n",
        ")\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TcgyZDoz_DI"
      },
      "source": [
        "## Callback"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLPahLlK0Awz"
      },
      "source": [
        "segmentation_classes = ['pet', 'pet_outline', 'background']\n",
        "\n",
        "# returns a dictionary of labels\n",
        "def labels():\n",
        "    l = {}\n",
        "    for i, label in enumerate(segmentation_classes):\n",
        "        l[i] = label\n",
        "    return l"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class WandbSemanticLogger(WandbEvalCallback):\n",
        "    def __init__(\n",
        "        self,\n",
        "        validloader,\n",
        "        data_table_columns=[\"index\", \"image\"],\n",
        "        pred_table_columns=[\"epoch\", \"index\", \"image\", \"prediction\"],\n",
        "        num_samples=100,\n",
        "    ):\n",
        "        super().__init__(\n",
        "            data_table_columns,\n",
        "            pred_table_columns,\n",
        "        )\n",
        "\n",
        "        self.val_data = validloader.unbatch().take(num_samples)\n",
        "\n",
        "    def add_ground_truth(self, logs):\n",
        "        for idx, (image, mask) in enumerate(self.val_data):\n",
        "            self.data_table.add_data(\n",
        "                idx,\n",
        "                self._prepare_wandb_mask(\n",
        "                    image.numpy(),\n",
        "                    np.argmax(mask.numpy(), axis=-1),\n",
        "                    \"ground_truth\"\n",
        "                )\n",
        "            )\n",
        "\n",
        "    def add_model_predictions(self, epoch, logs):\n",
        "        data_table_ref = self.data_table_ref\n",
        "        table_idxs = data_table_ref.get_index()\n",
        "\n",
        "        for idx, (image, mask) in enumerate(self.val_data):\n",
        "            prediction = self.model.predict(tf.expand_dims(image, axis=0), verbose=0)\n",
        "            prediction = np.argmax(tf.squeeze(prediction, axis=0).numpy(), axis=-1)\n",
        "\n",
        "            self.pred_table.add_data(\n",
        "                epoch,\n",
        "                data_table_ref.data[idx][0],\n",
        "                self._prepare_wandb_mask(\n",
        "                    data_table_ref.data[idx][1],\n",
        "                    np.argmax(mask.numpy(), axis=-1),\n",
        "                    \"ground_truth\"\n",
        "                ),\n",
        "                self._prepare_wandb_mask(\n",
        "                    data_table_ref.data[idx][1],\n",
        "                    prediction,\n",
        "                    \"prediction\"\n",
        "                )\n",
        "            )\n",
        "\n",
        "    def _prepare_wandb_mask(self, image, mask, mask_type):\n",
        "        return wandb.Image(\n",
        "            image,\n",
        "            masks = {\n",
        "                \"ground_truth\": {\n",
        "                    \"mask_data\": mask,\n",
        "                    \"class_labels\": labels()\n",
        "            }})"
      ],
      "metadata": {
        "id": "twjjzv9MN7N0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xe8JWIlXs_EQ"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odN9SwNqaPz3"
      },
      "source": [
        "run = wandb.init(project='image-segmentation', config=configs)\n",
        "\n",
        "_ = model.fit(\n",
        "    trainloader, \n",
        "    epochs=10, \n",
        "    validation_data=validloader,\n",
        "    callbacks=[\n",
        "        WandbMetricsLogger(log_freq=2),\n",
        "        WandbSemanticLogger(validloader)\n",
        "      ]\n",
        "    )\n",
        "\n",
        "run.finish()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}