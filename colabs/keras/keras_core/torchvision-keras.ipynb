{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLRorPqA1-h4"
      },
      "source": [
        "<img src=\"https://keras.io/img/logo-k-keras-wb.png\" width=\"200\" alt=\"Keras\" />\n",
        "<img src=\"https://wandb.me/logo-im-png\" width=\"400\" alt=\"Weights & Biases\" />\n",
        "<!--- @wandbcode{keras_core_torchvision} -->\n",
        "\n",
        "# ðŸ”¥ Fine-tune a TorchVision Model with Keras and WandB ðŸ¦„\n",
        "\n",
        "[![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/wandb/examples/blob/master/colabs/keras/keras_core/torchvision_keras.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEl-j2hq2w25"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "[TorchVision](https://pytorch.org/vision/stable/index.html) is a library part of the [PyTorch](http://pytorch.org/) project that consists of popular datasets, model architectures, and common image transformations for computer vision. This example demonstrates how we can perform transfer learning for image classification using a pre-trained backbone model from TorchVision on the [Imagenette dataset](https://github.com/fastai/imagenette) using KerasCore. We will also demonstrate the compatibility of KerasCore with an input system consisting of [Torch Datasets and Dataloaders](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html).\n",
        "\n",
        "### References:\n",
        "\n",
        "- [Customizing what happens in `fit()` with PyTorch](https://keras.io/keras_core/guides/custom_train_step_in_torch/)\n",
        "- [PyTorch Datasets and Dataloaders](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)\n",
        "- [Transfer learning for Computer Vision using PyTorch](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html)\n",
        "\n",
        "## Setup\n",
        "\n",
        "- We install the `main` branch of [KerasCore](https://github.com/keras-team/keras-core), this lets us use the latest feature merged in KerasCore.\n",
        "- We also install [wandb-addons](https://github.com/soumik12345/wandb-addons), a library that hosts the backend-agnostic callbacks compatible with KerasCore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4rfNRPgiy9v",
        "outputId": "ce5ba027-567a-4577-a638-ca8802ee1f84"
      },
      "outputs": [],
      "source": [
        "# install the `main` branch of KerasCore\n",
        "!pip install -qq namex\n",
        "!apt install python3.10-venv\n",
        "!git clone https://github.com/soumik12345/keras-core.git && cd keras-core && python pip_build.py --install\n",
        "\n",
        "# install wandb-addons\n",
        "!pip install -qq git+https://github.com/soumik12345/wandb-addons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7nudAUt8jHRB"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"KERAS_BACKEND\"] = \"torch\"\n",
        "\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "\n",
        "import keras_core as keras\n",
        "from keras_core.utils import TorchModuleWrapper\n",
        "\n",
        "import wandb\n",
        "from wandb_addons.keras import WandbMetricsLogger, WandbModelCheckpoint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pS1c-ySo7nty"
      },
      "source": [
        "## Define the Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ovtXSUA7ksk",
        "outputId": "725bef1a-0e68-473e-8c24-1f1ffd28506c"
      },
      "outputs": [],
      "source": [
        "wandb.init(project=\"keras-torch\", entity=\"ml-colabs\", job_type=\"torchvision/train\")\n",
        "\n",
        "config = wandb.config\n",
        "config.batch_size = 32\n",
        "config.image_size = 224\n",
        "config.initial_learning_rate = 1e-3\n",
        "config.num_epochs = 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5uU_Q_H74GO"
      },
      "source": [
        "## Creating the Torch Datasets and Dataloaders\n",
        "\n",
        "In this example, we would train an image classification model on the [Imagenette dataset](https://github.com/fastai/imagenette). Imagenette is a subset of 10 easily classified classes from [Imagenet](https://www.image-net.org/) (tench, English springer, cassette player, chain saw, church, French horn, garbage truck, gas pump, golf ball, parachute)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gWV2hNGo8vW7",
        "outputId": "cc70b598-fc66-480a-a98c-7077fa634a22"
      },
      "outputs": [],
      "source": [
        "# Fetch the imagenette dataset\n",
        "data_dir = keras.utils.get_file(\n",
        "    fname=\"imagenette2-320.tgz\",\n",
        "    origin=\"https://s3.amazonaws.com/fast-ai-imageclas/imagenette2-320.tgz\",\n",
        "    extract=True,\n",
        ")\n",
        "data_dir = data_dir.replace(\".tgz\", \"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ka7TDUMn9IcG"
      },
      "source": [
        "Next, we define pre-processing and augmentation transforms from TorchVision for the train and validation sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rFZKJafF9H6y"
      },
      "outputs": [],
      "source": [
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(config.image_size),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(config.image_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aE3VFQHm9srl"
      },
      "source": [
        "Finally, we will use TorchVision and the [`torch.utils.data`](https://pytorch.org/docs/stable/data.html) packages for creating the dataloaders for trainig and validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0N81UNjtjMZ4",
        "outputId": "bfb2af1a-977c-4408-cb1c-8da78d03d13d"
      },
      "outputs": [],
      "source": [
        "# Define the train and validation datasets\n",
        "image_datasets = {\n",
        "    x: datasets.ImageFolder(\n",
        "        os.path.join(data_dir, x), data_transforms[x]\n",
        "    )\n",
        "    for x in ['train', 'val']\n",
        "}\n",
        "\n",
        "# Define the torch dataloaders corresponding to the\n",
        "# train and validation dataset\n",
        "dataloaders = {\n",
        "    x: torch.utils.data.DataLoader(\n",
        "        image_datasets[x],\n",
        "        batch_size=config.batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=4\n",
        "    )\n",
        "    for x in ['train', 'val']\n",
        "}\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
        "class_names = image_datasets['train'].classes\n",
        "\n",
        "# Specify the global device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AY6kOIL--EdP"
      },
      "source": [
        "Let us visualize a few samples from the training dataloader."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 829
        },
        "id": "yffdD4LxjOQG",
        "outputId": "38e6f182-11b1-4830-e5ec-a5588c7bdbf9"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "sample_images, sample_labels = next(iter(dataloaders['train']))\n",
        "sample_images = sample_images.numpy()\n",
        "sample_labels = sample_labels.numpy()\n",
        "for idx in range(9):\n",
        "    ax = plt.subplot(3, 3, idx + 1)\n",
        "    image = sample_images[idx].transpose((1, 2, 0))\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    image = std * image + mean\n",
        "    image = np.clip(image, 0, 1)\n",
        "    plt.imshow(image)\n",
        "    plt.title(\"Ground Truth Label: \" + class_names[int(sample_labels[idx])])\n",
        "    plt.axis(\"off\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zvfzD04-ce9"
      },
      "source": [
        "## The Image Classification Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBLzXVwk-mLP"
      },
      "source": [
        "We typically define a model in PyTorch using [`torch.nn.Module`s](https://pytorch.org/docs/stable/notes/modules.html) which act as the building blocks of stateful computation. Let us define the ResNet18 model from the TorchVision package as a `torch.nn.Module` pre-trained on the [Imagenet1K dataset](https://huggingface.co/datasets/imagenet-1k)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOGUiI9K_BRk",
        "outputId": "97aa387c-7b3d-41cf-959b-de60eb572912"
      },
      "outputs": [],
      "source": [
        "# Define the pre-trained resnet18 module from TorchVision\n",
        "resnet_18 = models.resnet18(weights='IMAGENET1K_V1')\n",
        "\n",
        "# We set the classification head of the pre-trained ResNet18\n",
        "# module to an identity module\n",
        "resnet_18.fc = nn.Identity()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afxbMwcYF-Yz"
      },
      "source": [
        "ven though Keras supports PyTorch as a backend, it does not mean that we can nest torch modules inside a [`keras_core.Model`](https://keras.io/keras_core/api/models/), because trainable variables inside a Keras Model is tracked exclusively via [Keras Layers](https://keras.io/keras_core/api/layers/).\n",
        "\n",
        "KerasCore provides us with a feature called `TorchModuleWrapper` which enables us to do exactly this. The `TorchModuleWrapper` is a Keras Layer that accepts a torch module and tracks its trainable variables, essentially converting the torch module into a Keras Layer. This enables us to put any torch modules inside a Keras Model and train them with a single `model.fit()`!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JLuCIAy5F6L1"
      },
      "outputs": [],
      "source": [
        "# We set the trainable ResNet18 backbone to be a Keras Layer\n",
        "# using `TorchModuleWrapper`\n",
        "backbone = TorchModuleWrapper(resnet_18)\n",
        "\n",
        "# We set this to `False` if you want to freeze the backbone\n",
        "backbone.trainable = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7y28txoVHHk8"
      },
      "source": [
        "Now, we will build a Keras functional model with the backbone layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        },
        "id": "l2rxqA8vjR3W",
        "outputId": "206ad321-5fb5-41e1-a391-37cef3b10edb"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(3, config.image_size, config.image_size))\n",
        "x = backbone(inputs)\n",
        "x = keras.layers.Dropout(0.5)(x)\n",
        "x = keras.layers.Dense(len(class_names))(x)\n",
        "outputs = keras.activations.softmax(x, axis=1)\n",
        "model = keras.Model(inputs, outputs, name=\"ResNet18_Classifier\")\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BXbvYzDnjyDQ",
        "outputId": "9d7e51b2-85e7-4717-993b-0f6efac2999d"
      },
      "outputs": [],
      "source": [
        "# Create exponential decay learning rate scheduler\n",
        "decay_steps = config.num_epochs * len(dataloaders[\"train\"]) // config.batch_size\n",
        "lr_scheduler = keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate=config.initial_learning_rate,\n",
        "    decay_steps=decay_steps,\n",
        "    decay_rate=0.1,\n",
        ")\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    optimizer=keras.optimizers.Adam(lr_scheduler),\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "# Define the backend-agnostic WandB callbacks for KerasCore\n",
        "callbacks = [\n",
        "    # Track experiment metrics with WandB\n",
        "    WandbMetricsLogger(log_freq=\"batch\"),\n",
        "    # Save best model checkpoints to WandB\n",
        "    WandbModelCheckpoint(\n",
        "        filepath=\"model.weights.h5\",\n",
        "        monitor=\"val_loss\",\n",
        "        save_best_only=True,\n",
        "        save_weights_only=True,\n",
        "    )\n",
        "]\n",
        "\n",
        "# Train the model by calling model.fit\n",
        "history = model.fit(\n",
        "    dataloaders[\"train\"],\n",
        "    validation_data=dataloaders[\"val\"],\n",
        "    epochs=config.num_epochs,\n",
        "    callbacks=callbacks,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "upJf2M92JBlD",
        "outputId": "6993ffbe-2b69-4ff8-9a2f-9c502ca1414d"
      },
      "outputs": [],
      "source": [
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I58qvNnzJmHD"
      },
      "source": [
        "## Evaluation and Inference\n",
        "\n",
        "Now, we let us load the best model weights checkpoint and evaluate the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cNpfuELqnDI_",
        "outputId": "18bfe7bb-adea-4ac2-807b-924dce6fbcc8"
      },
      "outputs": [],
      "source": [
        "wandb.init(\n",
        "    project=\"keras-torch\", entity=\"ml-colabs\", job_type=\"torchvision/eval\"\n",
        ")\n",
        "artifact = wandb.use_artifact(\n",
        "    'ml-colabs/keras-torch/run_hiceci7f_model:latest', type='model'\n",
        ")\n",
        "artifact_dir = artifact.download()\n",
        "\n",
        "model.load_weights(os.path.join(artifact_dir, \"model.weights.h5\"))\n",
        "\n",
        "_, val_accuracy = model.evaluate(dataloaders[\"val\"])\n",
        "wandb.log({\"Validation-Accuracy\": val_accuracy})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vE2vAvBNKAI9"
      },
      "source": [
        "Finally, let us visualize the some predictions of the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "1b82953b5f134926ae8a11c4fedca385",
            "639fec0083134ab18b6a22999203536e",
            "5d85942dc6e44640aed593b7a8494493",
            "757ca7f811ec4db5a4dd517c5aec2bb8",
            "122edd2a0300448cac1fcc8645fd8708",
            "b1df464c3a2d47dbb49b36fa5d9912bb",
            "dd95b1ba48ec4bc5b494f319ad41eedf",
            "84dfa1f6d6f04c50b87e8683a0abdf55",
            "634e163cb1354824a764dc49f7d9f2fa",
            "5b1e8c0d14d84e0f923a997727794c89",
            "2e912ebb924c4269a07fb7c1204eb923"
          ]
        },
        "id": "ugrP307SpxMj",
        "outputId": "ead193f0-bfd9-4bbc-c675-d741d64fd70f"
      },
      "outputs": [],
      "source": [
        "table = wandb.Table(\n",
        "    columns=[\n",
        "        \"Image\", \"Ground-Truth\", \"Prediction\"\n",
        "    ] + [\"Confidence-\" + cls for cls in class_names]\n",
        ")\n",
        "\n",
        "sample_images, sample_labels = next(iter(dataloaders['train']))\n",
        "\n",
        "# We perform inference and detach the predicted probabilities from the Torch\n",
        "# computation graph with a tensor that does not require gradient computation.\n",
        "sample_pred_probas = model(sample_images.to(\"cuda\")).detach()\n",
        "sample_pred_logits = keras.ops.argmax(sample_pred_probas, axis=1)\n",
        "sample_pred_logits = sample_pred_logits.to(\"cpu\").numpy()\n",
        "sample_pred_probas = sample_pred_probas.to(\"cpu\").numpy()\n",
        "\n",
        "sample_images = sample_images.numpy()\n",
        "sample_labels = sample_labels.numpy()\n",
        "\n",
        "for idx in tqdm(range(sample_images.shape[0])):\n",
        "    image = sample_images[idx].transpose((1, 2, 0))\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    image = std * image + mean\n",
        "    image = np.clip(image, 0, 1)\n",
        "    table.add_data(\n",
        "        wandb.Image(image),\n",
        "        class_names[int(sample_labels[idx])],\n",
        "        class_names[int(sample_pred_logits[idx])],\n",
        "        *sample_pred_probas[idx].tolist(),\n",
        "    )\n",
        "\n",
        "wandb.log({\"Evaluation-Table\": table})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a90XmoRR65SJ",
        "outputId": "88e5b0b1-a3db-4366-ba71-f6a21a877676"
      },
      "outputs": [],
      "source": [
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QA6ytgUaSxsS"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "122edd2a0300448cac1fcc8645fd8708": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b82953b5f134926ae8a11c4fedca385": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_639fec0083134ab18b6a22999203536e",
              "IPY_MODEL_5d85942dc6e44640aed593b7a8494493",
              "IPY_MODEL_757ca7f811ec4db5a4dd517c5aec2bb8"
            ],
            "layout": "IPY_MODEL_122edd2a0300448cac1fcc8645fd8708"
          }
        },
        "2e912ebb924c4269a07fb7c1204eb923": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5b1e8c0d14d84e0f923a997727794c89": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d85942dc6e44640aed593b7a8494493": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_84dfa1f6d6f04c50b87e8683a0abdf55",
            "max": 32,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_634e163cb1354824a764dc49f7d9f2fa",
            "value": 32
          }
        },
        "634e163cb1354824a764dc49f7d9f2fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "639fec0083134ab18b6a22999203536e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b1df464c3a2d47dbb49b36fa5d9912bb",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_dd95b1ba48ec4bc5b494f319ad41eedf",
            "value": "100%"
          }
        },
        "757ca7f811ec4db5a4dd517c5aec2bb8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5b1e8c0d14d84e0f923a997727794c89",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_2e912ebb924c4269a07fb7c1204eb923",
            "value": " 32/32 [00:01&lt;00:00, 17.84it/s]"
          }
        },
        "84dfa1f6d6f04c50b87e8683a0abdf55": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b1df464c3a2d47dbb49b36fa5d9912bb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd95b1ba48ec4bc5b494f319ad41eedf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
