{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Yahoo Answers Classifier",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wandb/examples/blob/master/lightning/projects/yahoo-answers-classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQ_yA1SAVxJS"
      },
      "source": [
        "<img src=\"https://i.imgur.com/vnejHGh.png\" width=\"800\">\n",
        "<!--- @wandbcode{huggingface_tables} -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5G9Uv8ya80zb"
      },
      "source": [
        "# Yahoo! Answers Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eahh5ifUR_t9"
      },
      "source": [
        "## Installation and set-up"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBxbiGfsuvmU"
      },
      "source": [
        "%%capture\n",
        "!pip install transformers datasets wandb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puq9vfSISEAr"
      },
      "source": [
        "from datasets import load_dataset\n",
        "from datasets import load_metric\n",
        "import numpy as np\n",
        "import wandb\n",
        "\n",
        "wandb.login()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-NFxlBBZaHI"
      },
      "source": [
        "# Data preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4h3PLR9KaLSM"
      },
      "source": [
        "We'll download the\n",
        "[Yahoo! Answers Dataset](https://paperswithcode.com/sota/text-classification-on-yahoo-answers),\n",
        "which includes questions and answers from the now-defunct\n",
        "[Yahoo! Answers forum](https://en.wikipedia.org/wiki/Yahoo!_Answers).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GaST634Bvurt"
      },
      "source": [
        "dataset = load_dataset(\"yahoo_answers_topics\")\n",
        "dataset[\"train\"][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sIR8vlEW49V"
      },
      "source": [
        "We'll decrease the size of the dataset for faster logging."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKbQbX0eW_nP"
      },
      "source": [
        "dataset['test'] = dataset['test'].select(range(dataset['test'].num_rows // 10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ts5o9tyTyamN"
      },
      "source": [
        "The task is to predict the category of the question, AKA its `topic`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E08CcJw5GH3A"
      },
      "source": [
        "label_list = dataset['train'].unique('topic')\n",
        "num_labels = len(label_list)\n",
        "dataset = dataset.rename_column('topic', 'labels')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4r3JPRKI3xcY"
      },
      "source": [
        "# Training the model and logging to W&B\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPb2y1zl8_1L"
      },
      "source": [
        "This cell sets up logging of validation data during training,\n",
        "so that we can see model outputs and not just loss and accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPA3bl3_1xFk"
      },
      "source": [
        "from wandb.sdk.integration_utils.data_logging import ValidationDataLogger\n",
        "\n",
        "# automatically log model to W&B at the end\n",
        "%env WANDB_LOG_MODEL=true\n",
        "\n",
        "accuracy_metric = load_metric(\"accuracy\")\n",
        "\n",
        "validation_targets = [dataset['test'].features['labels'].int2str(x) for x in dataset['test']['labels']]\n",
        "validation_logger = ValidationDataLogger(inputs=dataset[\"test\"][:],\n",
        "                                         targets=validation_targets)\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "\n",
        "    # convert predictions from class (0, 1, 2, ...) to label (Health, Science‚Ä¶)\n",
        "    prediction_labels = [dataset['test'].features['labels'].int2str(x.item())\n",
        "                         for x in predictions]\n",
        "    \n",
        "    # log predictions\n",
        "    validation_logger.log_predictions(prediction_labels)\n",
        "\n",
        "    # metrics from the datasets library have a compute method\n",
        "    return accuracy_metric.compute(predictions=predictions, references=labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "riRlJlhkGKHc"
      },
      "source": [
        "###\n",
        "# TrainingArguments -- configure training and logging\n",
        "###\n",
        "\n",
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "run = wandb.init(entity=\"wandb\", project=\"yahoo-answers-topics-transformers\")\n",
        "\n",
        "args = TrainingArguments(  # docs: https://huggingface.co/transformers/main_classes/trainer.html#trainingarguments\n",
        "    report_to='wandb',                    # enable logging to W&B\n",
        "    output_dir='topic_classification',    # set output directory\n",
        "    overwrite_output_dir=True,            # is it okay to overwrite files there?\n",
        "    evaluation_strategy='steps',          # check evaluation metrics on a given # of steps\n",
        "    dataloader_num_workers=2,             # additional dataloading processes\n",
        "    logging_steps=50,                     # how often do we log?\n",
        "    logging_first_step=True,              # do we log at the start of training?\n",
        "    eval_steps=50,                        # how often do we run evaluation?\n",
        "    eval_accumulation_steps=1,            # how much do we accumulate between evaluations?\n",
        "    load_best_model_at_end=True,          # do we save the model at the end?\n",
        "    metric_for_best_model='accuracy',     # how do we judge the best model?\n",
        "    # hyperparameters\n",
        "    max_steps=500,                        # how long should we train for?\n",
        "    per_device_train_batch_size=8,        # batch size, increase to saturate GPU memory\n",
        "    learning_rate=5e-6,                   # optimizer learning rate, increase when you increase batch_size\n",
        "    weight_decay=0.,                      # weight decay regularization penalty\n",
        "    adam_epsilon=1e-8,                    # epsilon hyperparameter for Adam\n",
        "    adafactor=False,                      # use AdaFactor instead of AdamW\n",
        "    max_grad_norm=1.,                     # gradient clipping maximum\n",
        "    lr_scheduler_type=\"linear\",           # learning rate adjustment schedule\n",
        ")\n",
        "\n",
        "###\n",
        "# AutoModel and AutoTokenizer -- configure model\n",
        "###\n",
        "\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# model options here:\n",
        "#  https://huggingface.co/transformers/pretrained_models.html\n",
        "\n",
        "# try these: \"bert-base-uncased\", \"squeezebert/squeezebert-mnli-headless\",\n",
        "#            \"distilbert-base-uncased\", \"xlnet-base-cased\", \"distilroberta-base\"\n",
        "#            \"allenai/longformer-base-4096\",\n",
        "\n",
        "wandb.config[\"model_string\"] = \"distilbert-base-uncased\"\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(wandb.config.model_string, num_labels=num_labels)\n",
        "tokenizer = AutoTokenizer.from_pretrained(wandb.config.model_string)\n",
        "\n",
        "tokenized_dataset = dataset.map(lambda x: tokenizer(x['question_title'], truncation=True), batched=True)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model, args=args,\n",
        "    train_dataset=tokenized_dataset['train'],\n",
        "    eval_dataset=tokenized_dataset['test'],\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "###\n",
        "# üèÉ‚Äç‚ôÄÔ∏è Run Training üèÉ‚Äç‚ôÇÔ∏è\n",
        "###\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "run.finish()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}