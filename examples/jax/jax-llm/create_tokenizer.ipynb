{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe946e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import SentencePieceBPETokenizer, SentencePieceUnigramTokenizer\n",
    "from tokenizers.trainers import UnigramTrainer\n",
    "from tokenizers.processors import BertProcessing\n",
    "from transformers import PreTrainedTokenizerFast, PreTrainedTokenizer\n",
    "import datasets\n",
    "import pandas as pd\n",
    "from datasets import load_from_disk\n",
    "from pathlib import Path\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98df9b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(project='protobert', job_type=\"tokenizer_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a4368f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_at = run.use_artifact('uniref_1m:latest')\n",
    "dataset_dir = Path(data_at.download())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e68a2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dataset = load_from_disk(dataset_dir/'uniref_1m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb7b8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tokenizer = SentencePieceBPETokenizer()\n",
    "tokenizer.train_from_iterator(sample_dataset[\"text\"], vocab_size=1000, min_frequency=2, special_tokens=[\n",
    "    \"<s>\",\n",
    "    \"<pad>\",\n",
    "    \"</s>\",\n",
    "    \"<unk>\",\n",
    "    \"<mask>\",\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4d2d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd6d371",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save('proteins-tmp')\n",
    "tokenizer = PreTrainedTokenizerFast(tokenizer_file='proteins-tmp')\n",
    "tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "    (\"</s>\", 2),\n",
    "    (\"<s>\", 0),\n",
    ")\n",
    "tokenizer.mask_token = \"<mask>\"\n",
    "tokenizer.cls_token = \"</s>\"\n",
    "tokenizer.sep_token = \"<s>\"\n",
    "tokenizer.pad_token = \"<pad>\"\n",
    "tokenizer.unk_token = \"<unk>\"\n",
    "\n",
    "tokenizer.save_pretrained('proteins-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0884429",
   "metadata": {},
   "outputs": [],
   "source": [
    "o = tokenizer('ASDFAFDGADFGADFGHAG')\n",
    "tokenizer.decode(o['input_ids'])\n",
    "for i in o['input_ids']:\n",
    "    print(f'{i}: {tokenizer.decode(i)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcade17",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_at = wandb.Artifact('uniref_1m_tokenizer', type=\"tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc09cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_at.add_dir('proteins-base', name='uniref_1m_tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7a1c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "run.log_artifact(tok_at)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450f71bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "run.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
